[
  {
    "_id": {
      "$oid": "692a9087a6d7100bb5da9ec1"
    },
    "model_id": "command-r-plus-04-2024",
    "analysis_method": "Standardized Evaluation",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.378573+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 147,
    "normalized_score": 0.854,
    "score": 0.854,
    "self_reported_source_link": "https://huggingface.co/CohereForAI/c4ai-command-r-plus",
    "updated_at": "2025-07-19T19:56:11.378573+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9087a6d7100bb5da9ec8"
    },
    "model_id": "kimi-k2-base",
    "analysis_method": "EM",
    "benchmark_id": "triviaqa",
    "benchmark_name": "TriviaQA",
    "created_at": "2025-07-19T19:56:11.566226+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 243,
    "normalized_score": 0.851,
    "score": 0.851,
    "self_reported_source_link": "https://github.com/MoonshotAI/Kimi-K2",
    "updated_at": "2025-07-19T19:56:11.566226+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9087a6d7100bb5da9ed6"
    },
    "model_id": "kimi-k1.5",
    "analysis_method": "Pass@1",
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "created_at": "2025-07-19T19:56:12.132422+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 549,
    "normalized_score": 0.7,
    "score": 0.7,
    "self_reported_source_link": "https://github.com/MoonshotAI/Kimi-k1.5",
    "updated_at": "2025-07-19T19:56:12.132422+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9087a6d7100bb5da9ee0"
    },
    "model_id": "kimi-k2-0905",
    "analysis_method": null,
    "benchmark_id": "aime-2024",
    "benchmark_name": "AIME 2024",
    "created_at": "2024-09-05T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9006,
    "normalized_score": 0.72,
    "score": 0.72,
    "self_reported_source_link": "https://moonshot.cn/blog/kimi-k2-0905",
    "updated_at": "2024-09-15T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9087a6d7100bb5da9ee7"
    },
    "model_id": "kimi-k2-instruct-0905",
    "analysis_method": "2024/11/25 Pass@1",
    "benchmark_id": "livebench",
    "benchmark_name": "Livebench",
    "created_at": "2025-09-05T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 10029,
    "normalized_score": 0.764,
    "score": 0.764,
    "self_reported_source_link": "https://moonshotai.github.io/Kimi-K2/",
    "updated_at": "2025-09-15T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9087a6d7100bb5da9f05"
    },
    "model_id": "kimi-k2-instruct",
    "analysis_method": "Accuracy",
    "benchmark_id": "zebralogic",
    "benchmark_name": "ZebraLogic",
    "created_at": "2025-07-19T19:56:12.502879+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 714,
    "normalized_score": 0.89,
    "score": 0.89,
    "self_reported_source_link": "https://moonshotai.github.io/Kimi-K2/",
    "updated_at": "2025-07-19T19:56:12.502879+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9f2c"
    },
    "model_id": "nova-lite",
    "analysis_method": "composite step accuracy",
    "benchmark_id": "visualwebbench",
    "benchmark_name": "VisualWebBench",
    "created_at": "2025-07-19T19:56:12.750738+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 837,
    "normalized_score": 0.777,
    "score": 0.777,
    "self_reported_source_link": "https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card",
    "updated_at": "2025-07-19T19:56:12.750738+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9f48"
    },
    "model_id": "nova-pro",
    "analysis_method": "composite",
    "benchmark_id": "visualwebbench",
    "benchmark_name": "VisualWebBench",
    "created_at": "2025-07-19T19:56:12.752533+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 838,
    "normalized_score": 0.797,
    "score": 0.797,
    "self_reported_source_link": "https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card",
    "updated_at": "2025-07-19T19:56:12.752533+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9f64"
    },
    "model_id": "nova-micro",
    "analysis_method": "spBleu",
    "benchmark_id": "translation-set1\u2192en-spbleu",
    "benchmark_name": "Translation Set1\u2192en spBleu",
    "created_at": "2025-07-19T19:56:12.973209+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 935,
    "normalized_score": 0.426,
    "score": 0.426,
    "self_reported_source_link": "https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card",
    "updated_at": "2025-07-19T19:56:12.973209+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9f76"
    },
    "model_id": "gemma-3n-e4b",
    "analysis_method": "5-shot",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:13.207598+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1057,
    "normalized_score": 0.717,
    "score": 0.717,
    "self_reported_source_link": "https://huggingface.co/google/gemma-3n-E4B",
    "updated_at": "2025-07-19T19:56:13.207598+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9f82"
    },
    "model_id": "gemma-3-12b-it",
    "analysis_method": "0-shot evaluation",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.578915+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1227,
    "normalized_score": 0.516,
    "score": 0.516,
    "self_reported_source_link": "https://ai.google.dev/gemma/docs/core/model_card_3",
    "updated_at": "2025-07-19T19:56:13.578915+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9f9d"
    },
    "model_id": "gemma-3n-e4b-it-litert-preview",
    "analysis_method": "ChrF, 0-shot Character-level F-score",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.580409+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1228,
    "normalized_score": 0.501,
    "score": 0.501,
    "self_reported_source_link": "https://huggingface.co/google/gemma-3n-E4B-it-litert-preview",
    "updated_at": "2025-07-19T19:56:13.580409+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9fba"
    },
    "model_id": "gemini-2.5-pro",
    "analysis_method": "Accuracy",
    "benchmark_id": "video-mme",
    "benchmark_name": "Video-MME",
    "created_at": "2025-07-19T19:56:13.904547+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1379,
    "normalized_score": 0.848,
    "score": 0.848,
    "self_reported_source_link": "https://deepmind.google/models/gemini/pro/",
    "updated_at": "2025-07-19T19:56:13.904547+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9fcb"
    },
    "model_id": "gemini-1.0-pro",
    "analysis_method": "Accuracy",
    "benchmark_id": "wmt23",
    "benchmark_name": "WMT23",
    "created_at": "2025-07-19T19:56:13.937549+00:00",
    "is_self_reported": false,
    "model_benchmark_id": 1393,
    "normalized_score": 0.717,
    "score": 0.717,
    "self_reported_source_link": "https://example.com/benchmark-image",
    "updated_at": "2025-07-19T19:56:13.937549+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9fd5"
    },
    "model_id": "gemma-3n-e2b-it-litert-preview",
    "analysis_method": "ChrF, 0-shot Character-level F-score",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.582347+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1229,
    "normalized_score": 0.427,
    "score": 0.427,
    "self_reported_source_link": "https://huggingface.co/google/gemma-3n-E2B-it-litert-preview",
    "updated_at": "2025-07-19T19:56:13.582347+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5da9ff2"
    },
    "model_id": "gemini-2.0-flash-lite",
    "analysis_method": "Factuality",
    "benchmark_id": "simpleqa",
    "benchmark_name": "SimpleQA",
    "created_at": "2025-07-19T19:56:11.535234+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 226,
    "normalized_score": 0.217,
    "score": 0.217,
    "self_reported_source_link": "https://ai.google.dev/gemini-api/docs/models",
    "updated_at": "2025-07-19T19:56:11.535234+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa000"
    },
    "model_id": "gemini-2.5-flash-lite",
    "analysis_method": "Reka",
    "benchmark_id": "vibe-eval",
    "benchmark_name": "Vibe-Eval",
    "created_at": "2025-07-19T19:56:13.875989+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1365,
    "normalized_score": 0.513,
    "score": 0.513,
    "self_reported_source_link": "https://deepmind.google/models/gemini/flash-lite/",
    "updated_at": "2025-07-19T19:56:13.875989+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa00e"
    },
    "model_id": "gemma-2-9b-it",
    "analysis_method": "partial score evaluation",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.380497+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 148,
    "normalized_score": 0.806,
    "score": 0.806,
    "self_reported_source_link": "https://huggingface.co/blog/gemma2",
    "updated_at": "2025-07-19T19:56:11.380497+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa01f"
    },
    "model_id": "gemini-1.5-pro",
    "analysis_method": "Safety Compliance",
    "benchmark_id": "xstest",
    "benchmark_name": "XSTest",
    "created_at": "2025-07-19T19:56:14.002222+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1418,
    "normalized_score": 0.988,
    "score": 0.988,
    "self_reported_source_link": "https://deepmind.google/technologies/gemini/pro/",
    "updated_at": "2025-07-19T19:56:14.002222+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa037"
    },
    "model_id": "gemma-2-27b-it",
    "analysis_method": "5-shot",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:13.212219+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1060,
    "normalized_score": 0.837,
    "score": 0.837,
    "self_reported_source_link": "https://huggingface.co/blog/gemma2",
    "updated_at": "2025-07-19T19:56:13.212219+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa048"
    },
    "model_id": "gemini-2.5-pro-preview-06-05",
    "analysis_method": "Video understanding",
    "benchmark_id": "videommmu",
    "benchmark_name": "VideoMMMU",
    "created_at": "2025-07-19T19:56:14.009959+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1421,
    "normalized_score": 0.836,
    "score": 0.836,
    "self_reported_source_link": "https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/",
    "updated_at": "2025-07-19T19:56:14.009959+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa056"
    },
    "model_id": "gemini-2.5-flash",
    "analysis_method": "Accuracy",
    "benchmark_id": "vibe-eval",
    "benchmark_name": "Vibe-Eval",
    "created_at": "2025-07-19T19:56:13.880772+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1368,
    "normalized_score": 0.654,
    "score": 0.654,
    "self_reported_source_link": "https://developers.googleblog.com/en/start-building-with-gemini-25-flash/",
    "updated_at": "2025-07-19T19:56:13.880772+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa065"
    },
    "model_id": "gemini-2.0-flash-thinking",
    "analysis_method": "Image-text QA across various domains",
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "created_at": "2025-07-19T19:56:12.151038+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 559,
    "normalized_score": 0.754,
    "score": 0.754,
    "self_reported_source_link": "https://ai.google.dev/gemini-api/docs/models/gemini#evaluation",
    "updated_at": "2025-07-19T19:56:12.151038+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa069"
    },
    "model_id": "gemma-3n-e2b",
    "analysis_method": "5-shot",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:13.213740+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1061,
    "normalized_score": 0.668,
    "score": 0.668,
    "self_reported_source_link": "https://huggingface.co/google/gemma-3n-E2B",
    "updated_at": "2025-07-19T19:56:13.213740+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa075"
    },
    "model_id": "medgemma-4b-it",
    "analysis_method": "Tokenized F1",
    "benchmark_id": "vqa-rad",
    "benchmark_name": "VQA-Rad",
    "created_at": "2025-07-19T19:56:14.035504+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1428,
    "normalized_score": 0.499,
    "score": 0.499,
    "self_reported_source_link": "https://huggingface.co/google/medgemma-4b-it",
    "updated_at": "2025-07-19T19:56:14.035504+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa07d"
    },
    "model_id": "gemini-1.5-flash",
    "analysis_method": "Accuracy",
    "benchmark_id": "xstest",
    "benchmark_name": "XSTest",
    "created_at": "2025-07-19T19:56:14.004109+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1419,
    "normalized_score": 0.97,
    "score": 0.97,
    "self_reported_source_link": "https://deepmind.google/technologies/gemini/flash/",
    "updated_at": "2025-07-19T19:56:14.004109+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa094"
    },
    "model_id": "gemma-3n-e4b-it",
    "analysis_method": "Character-level F-score. 0-shot.",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.584588+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1230,
    "normalized_score": 0.501,
    "score": 0.501,
    "self_reported_source_link": "https://huggingface.co/google/gemma-3n-E4B-it",
    "updated_at": "2025-07-19T19:56:13.584588+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa0a7"
    },
    "model_id": "gemma-3-4b-it",
    "analysis_method": "0-shot evaluation",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.586157+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1231,
    "normalized_score": 0.468,
    "score": 0.468,
    "self_reported_source_link": "https://ai.google.dev/gemma/docs/core/model_card_3",
    "updated_at": "2025-07-19T19:56:13.586157+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa0c2"
    },
    "model_id": "gemma-3-27b-it",
    "analysis_method": "0-shot evaluation",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.587542+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1232,
    "normalized_score": 0.534,
    "score": 0.534,
    "self_reported_source_link": "https://ai.google.dev/gemma/docs/core/model_card_3",
    "updated_at": "2025-07-19T19:56:13.587542+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa0dd"
    },
    "model_id": "gemma-3-1b-it",
    "analysis_method": "0-shot evaluation",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.590063+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1233,
    "normalized_score": 0.359,
    "score": 0.359,
    "self_reported_source_link": "https://ai.google.dev/gemma/docs/core/model_card_3",
    "updated_at": "2025-07-19T19:56:13.590063+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa0f0"
    },
    "model_id": "gemini-1.5-flash-8b",
    "analysis_method": "Safe request fulfillment rate",
    "benchmark_id": "xstest",
    "benchmark_name": "XSTest",
    "created_at": "2025-07-19T19:56:14.005888+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1420,
    "normalized_score": 0.926,
    "score": 0.926,
    "self_reported_source_link": "https://deepmind.google/technologies/gemini/flash/",
    "updated_at": "2025-07-19T19:56:14.005888+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa0fe"
    },
    "model_id": "gemini-diffusion",
    "analysis_method": "pass @1, Non-agentic evaluation (single turn edit only), max prompt length of 32K",
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "created_at": "2025-07-19T19:56:13.824708+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1342,
    "normalized_score": 0.229,
    "score": 0.229,
    "self_reported_source_link": "https://deepmind.google/models/gemini-diffusion/",
    "updated_at": "2025-07-19T19:56:13.824708+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa109"
    },
    "model_id": "gemini-2.0-flash",
    "analysis_method": "Visual understanding in chat models with challenging everyday examples",
    "benchmark_id": "vibe-eval",
    "benchmark_name": "Vibe-Eval",
    "created_at": "2025-07-19T19:56:13.886575+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1371,
    "normalized_score": 0.563,
    "score": 0.563,
    "self_reported_source_link": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/",
    "updated_at": "2025-07-19T19:56:13.886575+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa117"
    },
    "model_id": "gemma-3n-e2b-it",
    "analysis_method": "Character-level F-score. 0-shot.",
    "benchmark_id": "wmt24++",
    "benchmark_name": "WMT24++",
    "created_at": "2025-07-19T19:56:13.592107+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1234,
    "normalized_score": 0.427,
    "score": 0.427,
    "self_reported_source_link": "https://huggingface.co/google/gemma-3n-E2B-it",
    "updated_at": "2025-07-19T19:56:13.592107+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa12a"
    },
    "model_id": "phi-4-mini-reasoning",
    "analysis_method": "Standard evaluation",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.032863+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 494,
    "normalized_score": 0.946,
    "score": 0.946,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-4-mini-reasoning",
    "updated_at": "2025-07-19T19:56:12.032863+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa12e"
    },
    "model_id": "phi-4",
    "analysis_method": "simple-evals",
    "benchmark_id": "simpleqa",
    "benchmark_name": "SimpleQA",
    "created_at": "2025-07-19T19:56:11.546523+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 233,
    "normalized_score": 0.03,
    "score": 0.03,
    "self_reported_source_link": "https://arxiv.org/pdf/2412.08905",
    "updated_at": "2025-07-19T19:56:11.546523+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa13c"
    },
    "model_id": "phi-4-mini",
    "analysis_method": "5-shot",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.382335+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 149,
    "normalized_score": 0.67,
    "score": 0.67,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-4-mini-instruct",
    "updated_at": "2025-07-19T19:56:11.382335+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa14e"
    },
    "model_id": "phi-3.5-moe-instruct",
    "analysis_method": "5-shot",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:13.215763+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1062,
    "normalized_score": 0.813,
    "score": 0.813,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-3.5-MoE-instruct",
    "updated_at": "2025-07-19T19:56:13.215763+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa16e"
    },
    "model_id": "phi-3.5-mini-instruct",
    "analysis_method": "5-shot",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:13.217697+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1063,
    "normalized_score": 0.685,
    "score": 0.685,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
    "updated_at": "2025-07-19T19:56:13.217697+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa18e"
    },
    "model_id": "phi-3.5-vision-instruct",
    "analysis_method": "standard evaluation",
    "benchmark_id": "textvqa",
    "benchmark_name": "TextVQA",
    "created_at": "2025-07-19T19:56:12.888892+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 906,
    "normalized_score": 0.72,
    "score": 0.72,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-3.5-vision-instruct",
    "updated_at": "2025-07-19T19:56:12.888892+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa198"
    },
    "model_id": "phi-4-reasoning-plus",
    "analysis_method": "2.21",
    "benchmark_id": "phibench",
    "benchmark_name": "PhiBench",
    "created_at": "2025-07-19T19:56:14.126449+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1467,
    "normalized_score": 0.742,
    "score": 0.742,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-4-reasoning-plus",
    "updated_at": "2025-07-19T19:56:14.126449+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa1a4"
    },
    "model_id": "phi-4-multimodal-instruct",
    "analysis_method": "16 frames",
    "benchmark_id": "video-mme",
    "benchmark_name": "Video-MME",
    "created_at": "2025-07-19T19:56:13.911859+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1383,
    "normalized_score": 0.55,
    "score": 0.55,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct",
    "updated_at": "2025-07-19T19:56:13.911859+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa1b4"
    },
    "model_id": "phi-4-reasoning",
    "analysis_method": "2.21",
    "benchmark_id": "phibench",
    "benchmark_name": "PhiBench",
    "created_at": "2025-07-19T19:56:14.127989+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1468,
    "normalized_score": 0.706,
    "score": 0.706,
    "self_reported_source_link": "https://huggingface.co/microsoft/Phi-4-reasoning-plus",
    "updated_at": "2025-07-19T19:56:14.127989+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa1c0"
    },
    "model_id": "llama-3.1-70b-instruct",
    "analysis_method": "0-shot",
    "benchmark_id": "nexus",
    "benchmark_name": "Nexus",
    "created_at": "2025-07-19T19:56:14.394299+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1566,
    "normalized_score": 0.567,
    "score": 0.567,
    "self_reported_source_link": "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct",
    "updated_at": "2025-07-19T19:56:14.394299+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa1d3"
    },
    "model_id": "llama-3.2-90b-instruct",
    "analysis_method": "-",
    "benchmark_id": "vqav2",
    "benchmark_name": "VQAv2",
    "created_at": "2025-07-19T19:56:14.412800+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1573,
    "normalized_score": 0.781,
    "score": 0.781,
    "self_reported_source_link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
    "updated_at": "2025-07-19T19:56:14.412800+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa1e1"
    },
    "model_id": "llama-3.1-8b-instruct",
    "analysis_method": "0-shot",
    "benchmark_id": "nexus",
    "benchmark_name": "Nexus",
    "created_at": "2025-07-19T19:56:14.396611+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1567,
    "normalized_score": 0.385,
    "score": 0.385,
    "self_reported_source_link": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
    "updated_at": "2025-07-19T19:56:14.396611+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa1f4"
    },
    "model_id": "llama-3.1-405b-instruct",
    "analysis_method": "0-shot, macro_avg/acc",
    "benchmark_id": "nexus",
    "benchmark_name": "Nexus",
    "created_at": "2025-07-19T19:56:14.398966+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1568,
    "normalized_score": 0.587,
    "score": 0.587,
    "self_reported_source_link": "https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct",
    "updated_at": "2025-07-19T19:56:14.398966+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa207"
    },
    "model_id": "llama-3.2-11b-instruct",
    "analysis_method": "Accuracy",
    "benchmark_id": "vqav2-(test)",
    "benchmark_name": "VQAv2 (test)",
    "created_at": "2025-07-19T19:56:14.434081+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1580,
    "normalized_score": 0.752,
    "score": 0.752,
    "self_reported_source_link": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct",
    "updated_at": "2025-07-19T19:56:14.434081+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa213"
    },
    "model_id": "llama-3.2-3b-instruct",
    "analysis_method": "1-shot, rougeL",
    "benchmark_id": "tldr9+-(test)",
    "benchmark_name": "TLDR9+ (test)",
    "created_at": "2025-07-19T19:56:14.443142+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1582,
    "normalized_score": 0.19,
    "score": 0.19,
    "self_reported_source_link": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
    "updated_at": "2025-07-19T19:56:14.443142+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa223"
    },
    "model_id": "llama-4-maverick",
    "analysis_method": "1-shot average/f1",
    "benchmark_id": "tydiqa",
    "benchmark_name": "TydiQA",
    "created_at": "2025-07-19T19:56:14.475429+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1591,
    "normalized_score": 0.317,
    "score": 0.317,
    "self_reported_source_link": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "updated_at": "2025-07-19T19:56:14.475429+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa231"
    },
    "model_id": "llama-4-scout",
    "analysis_method": "1-shot average/f1",
    "benchmark_id": "tydiqa",
    "benchmark_name": "TydiQA",
    "created_at": "2025-07-19T19:56:14.477364+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1592,
    "normalized_score": 0.315,
    "score": 0.315,
    "self_reported_source_link": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "updated_at": "2025-07-19T19:56:14.477364+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa23e"
    },
    "model_id": "llama-3.3-70b-instruct",
    "analysis_method": "0-shot CoT",
    "benchmark_id": "mmlu-pro",
    "benchmark_name": "MMLU-Pro",
    "created_at": "2025-07-19T19:56:11.463251+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 189,
    "normalized_score": 0.689,
    "score": 0.689,
    "self_reported_source_link": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md",
    "updated_at": "2025-07-19T19:56:11.463251+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa248"
    },
    "model_id": "qwen-2.5-32b-instruct",
    "analysis_method": "Winogrande benchmark evaluation",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.384431+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 150,
    "normalized_score": 0.82,
    "score": 0.82,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "updated_at": "2025-07-19T19:56:11.384431+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa25b"
    },
    "model_id": "qwen-2.5-coder-32b-instruct",
    "analysis_method": "accuracy",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:13.219435+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1064,
    "normalized_score": 0.808,
    "score": 0.808,
    "self_reported_source_link": "https://arxiv.org/abs/2409.12186",
    "updated_at": "2025-07-19T19:56:13.219435+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa26b"
    },
    "model_id": "qwen3-235b-a22b-instruct-2507",
    "analysis_method": "Accuracy",
    "benchmark_id": "zebralogic",
    "benchmark_name": "ZebraLogic",
    "created_at": "2025-08-03T22:06:13.659618+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 15996,
    "normalized_score": 0.95,
    "score": 0.95,
    "self_reported_source_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
    "updated_at": "2025-08-03T22:06:13.659618+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa285"
    },
    "model_id": "qwq-32b",
    "analysis_method": "accuracy",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.034467+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 495,
    "normalized_score": 0.906,
    "score": 0.906,
    "self_reported_source_link": "https://qwen-ai.com/qwq-32b/",
    "updated_at": "2025-07-19T19:56:12.034467+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa28d"
    },
    "model_id": "qwen3-235b-a22b-thinking-2507",
    "analysis_method": null,
    "benchmark_id": "polymath",
    "benchmark_name": "PolyMATH",
    "created_at": "2025-07-25T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9125,
    "normalized_score": 0.601,
    "score": 0.601,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen3-thinking/",
    "updated_at": "2025-09-15T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa2a7"
    },
    "model_id": "qwen2-7b-instruct",
    "analysis_method": "Accuracy",
    "benchmark_id": "theoremqa",
    "benchmark_name": "TheoremQA",
    "created_at": "2025-07-19T19:56:14.487702+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1595,
    "normalized_score": 0.253,
    "score": 0.253,
    "self_reported_source_link": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
    "updated_at": "2025-07-19T19:56:14.487702+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa2b6"
    },
    "model_id": "qwq-32b-preview",
    "analysis_method": "accuracy",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.036449+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 496,
    "normalized_score": 0.906,
    "score": 0.906,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwq-32b-preview/",
    "updated_at": "2025-07-19T19:56:12.036449+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa2bb"
    },
    "model_id": "qwen3-next-80b-a3b-thinking",
    "analysis_method": null,
    "benchmark_id": "polymath",
    "benchmark_name": "PolyMATH",
    "created_at": "2025-01-10T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9223,
    "normalized_score": 0.563,
    "score": 0.563,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen3-next/",
    "updated_at": "2025-09-15T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa2d3"
    },
    "model_id": "qwen-2.5-coder-7b-instruct",
    "analysis_method": "accuracy",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:13.221874+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1065,
    "normalized_score": 0.729,
    "score": 0.729,
    "self_reported_source_link": "https://arxiv.org/abs/2409.12186",
    "updated_at": "2025-07-19T19:56:13.221874+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa2e7"
    },
    "model_id": "qwen-2.5-14b-instruct",
    "analysis_method": "TruthfulQA benchmark evaluation",
    "benchmark_id": "truthfulqa",
    "benchmark_name": "TruthfulQA",
    "created_at": "2025-07-19T19:56:11.355004+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 138,
    "normalized_score": 0.584,
    "score": 0.584,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "updated_at": "2025-07-19T19:56:11.355004+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa2f8"
    },
    "model_id": "qwen2-vl-72b",
    "analysis_method": "score",
    "benchmark_id": "vcr-en-easy",
    "benchmark_name": "VCR_en_easy",
    "created_at": "2025-07-19T19:56:14.594379+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1632,
    "normalized_score": 0.9193,
    "score": 0.9193,
    "self_reported_source_link": "https://github.com/QwenLM/Qwen2",
    "updated_at": "2025-07-19T19:56:14.594379+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa308"
    },
    "model_id": "qwen3-32b",
    "analysis_method": "Accuracy",
    "benchmark_id": "multilf",
    "benchmark_name": "MultiLF",
    "created_at": "2025-07-19T19:56:14.630716+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1646,
    "normalized_score": 0.73,
    "score": 0.73,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen3/",
    "updated_at": "2025-07-19T19:56:14.630716+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa312"
    },
    "model_id": "qwen3-235b-a22b",
    "analysis_method": "Accuracy",
    "benchmark_id": "supergpqa",
    "benchmark_name": "SuperGPQA",
    "created_at": "2025-07-19T19:56:11.784624+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 366,
    "normalized_score": 0.4406,
    "score": 0.4406,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen3/",
    "updated_at": "2025-07-19T19:56:11.784624+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa32a"
    },
    "model_id": "qwen-2.5-72b-instruct",
    "analysis_method": "MultiPL-E benchmark evaluation",
    "benchmark_id": "multipl-e",
    "benchmark_name": "MultiPL-E",
    "created_at": "2025-07-19T19:56:12.322800+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 644,
    "normalized_score": 0.751,
    "score": 0.751,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen2.5/",
    "updated_at": "2025-07-19T19:56:12.322800+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa339"
    },
    "model_id": "qwen3-30b-a3b",
    "analysis_method": "Accuracy",
    "benchmark_id": "multi-if",
    "benchmark_name": "Multi-IF",
    "created_at": "2025-07-19T19:56:14.641584+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1649,
    "normalized_score": 0.722,
    "score": 0.722,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen3/",
    "updated_at": "2025-07-19T19:56:14.641584+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa342"
    },
    "model_id": "qwen2.5-vl-7b",
    "analysis_method": "Score",
    "benchmark_id": "videomme-w-sub.",
    "benchmark_name": "VideoMME w sub.",
    "created_at": "2025-07-19T19:56:14.726358+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1684,
    "normalized_score": 0.716,
    "score": 0.716,
    "self_reported_source_link": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
    "updated_at": "2025-07-19T19:56:14.726358+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa363"
    },
    "model_id": "qvq-72b-preview",
    "analysis_method": "full",
    "benchmark_id": "olympiadbench",
    "benchmark_name": "OlympiadBench",
    "created_at": "2025-07-19T19:56:14.824642+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1716,
    "normalized_score": 0.204,
    "score": 0.204,
    "self_reported_source_link": "https://huggingface.co/Qwen/QVQ-72B-Preview",
    "updated_at": "2025-07-19T19:56:14.824642+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa368"
    },
    "model_id": "qwen2.5-omni-7b",
    "analysis_method": "Score",
    "benchmark_id": "voicebench-avg",
    "benchmark_name": "VoiceBench Avg",
    "created_at": "2025-07-19T19:56:14.925208+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1743,
    "normalized_score": 0.7412,
    "score": 0.7412,
    "self_reported_source_link": "https://github.com/QwenLM/Qwen2.5-Omni",
    "updated_at": "2025-07-19T19:56:14.925208+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa396"
    },
    "model_id": "qwen-2.5-7b-instruct",
    "analysis_method": "MultiPL-E benchmark evaluation",
    "benchmark_id": "multipl-e",
    "benchmark_name": "MultiPL-E",
    "created_at": "2025-07-19T19:56:12.325846+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 646,
    "normalized_score": 0.704,
    "score": 0.704,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "updated_at": "2025-07-19T19:56:12.325846+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa3a5"
    },
    "model_id": "qwen3-next-80b-a3b-instruct",
    "analysis_method": null,
    "benchmark_id": "polymath",
    "benchmark_name": "PolyMATH",
    "created_at": "2025-01-10T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9324,
    "normalized_score": 0.459,
    "score": 0.459,
    "self_reported_source_link": "https://qwenlm.github.io/blog/qwen3-next/",
    "updated_at": "2025-09-15T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa3be"
    },
    "model_id": "qwen2.5-vl-72b",
    "analysis_method": "Score",
    "benchmark_id": "videomme-w-o-sub.",
    "benchmark_name": "VideoMME w/o sub.",
    "created_at": "2025-07-19T19:56:14.720259+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1682,
    "normalized_score": 0.733,
    "score": 0.733,
    "self_reported_source_link": "https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct",
    "updated_at": "2025-07-19T19:56:14.720259+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa3dd"
    },
    "model_id": "qwen2-72b-instruct",
    "analysis_method": "Accuracy",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.386216+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 151,
    "normalized_score": 0.851,
    "score": 0.851,
    "self_reported_source_link": "https://huggingface.co/Qwen/Qwen2-72B",
    "updated_at": "2025-07-19T19:56:11.386216+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa3ef"
    },
    "model_id": "qwen2.5-vl-32b",
    "analysis_method": "Score",
    "benchmark_id": "videomme-w-sub.",
    "benchmark_name": "VideoMME w sub.",
    "created_at": "2025-07-19T19:56:14.729388+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1686,
    "normalized_score": 0.779,
    "score": 0.779,
    "self_reported_source_link": "https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct",
    "updated_at": "2025-07-19T19:56:14.729388+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa40c"
    },
    "model_id": "deepseek-r1-0528",
    "analysis_method": "Pass@1, Thinking mode",
    "benchmark_id": "hmmt-2025",
    "benchmark_name": "HMMT 2025",
    "created_at": "2025-05-28T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9616,
    "normalized_score": 0.794,
    "score": 0.794,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
    "updated_at": "2025-09-15T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa41d"
    },
    "model_id": "deepseek-vl2",
    "analysis_method": "val",
    "benchmark_id": "textvqa",
    "benchmark_name": "TextVQA",
    "created_at": "2025-07-19T19:56:12.902069+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 912,
    "normalized_score": 0.842,
    "score": 0.842,
    "self_reported_source_link": "https://arxiv.org/pdf/2412.10302",
    "updated_at": "2025-07-19T19:56:12.902069+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa42c"
    },
    "model_id": "deepseek-vl2-tiny",
    "analysis_method": "val",
    "benchmark_id": "textvqa",
    "benchmark_name": "TextVQA",
    "created_at": "2025-07-19T19:56:12.904238+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 913,
    "normalized_score": 0.807,
    "score": 0.807,
    "self_reported_source_link": "https://arxiv.org/pdf/2412.10302",
    "updated_at": "2025-07-19T19:56:12.904238+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa43b"
    },
    "model_id": "deepseek-r1-zero",
    "analysis_method": "Pass@1",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.038172+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 497,
    "normalized_score": 0.959,
    "score": 0.959,
    "self_reported_source_link": "https://arxiv.org/abs/2501.12948",
    "updated_at": "2025-07-19T19:56:12.038172+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa440"
    },
    "model_id": "deepseek-vl2-small",
    "analysis_method": "val",
    "benchmark_id": "textvqa",
    "benchmark_name": "TextVQA",
    "created_at": "2025-07-19T19:56:12.906237+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 914,
    "normalized_score": 0.834,
    "score": 0.834,
    "self_reported_source_link": "https://arxiv.org/pdf/2412.10302",
    "updated_at": "2025-07-19T19:56:12.906237+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa44f"
    },
    "model_id": "deepseek-r1-distill-qwen-7b",
    "analysis_method": "Pass@1",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.039853+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 498,
    "normalized_score": 0.928,
    "score": 0.928,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "updated_at": "2025-07-19T19:56:12.039853+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa454"
    },
    "model_id": "deepseek-r1-distill-qwen-1.5b",
    "analysis_method": "Pass@1",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.041592+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 499,
    "normalized_score": 0.839,
    "score": 0.839,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "updated_at": "2025-07-19T19:56:12.041592+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa459"
    },
    "model_id": "deepseek-v3",
    "analysis_method": "Resolved",
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "created_at": "2025-07-19T19:56:13.828562+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1344,
    "normalized_score": 0.42,
    "score": 0.42,
    "self_reported_source_link": "https://github.com/deepseek-ai/DeepSeek-V3",
    "updated_at": "2025-07-19T19:56:13.828562+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa46e"
    },
    "model_id": "deepseek-v2.5",
    "analysis_method": "Score",
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "created_at": "2025-07-19T19:56:13.830793+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1345,
    "normalized_score": 0.168,
    "score": 0.168,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
    "updated_at": "2025-07-19T19:56:13.830793+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa47e"
    },
    "model_id": "deepseek-r1-distill-llama-8b",
    "analysis_method": "Pass@1",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.046427+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 502,
    "normalized_score": 0.891,
    "score": 0.891,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
    "updated_at": "2025-07-19T19:56:12.046427+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa483"
    },
    "model_id": "deepseek-r1-distill-llama-70b",
    "analysis_method": "Pass@1",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.048302+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 503,
    "normalized_score": 0.945,
    "score": 0.945,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "updated_at": "2025-07-19T19:56:12.048302+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa488"
    },
    "model_id": "deepseek-r1-distill-qwen-14b",
    "analysis_method": "Pass@1",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.050287+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 504,
    "normalized_score": 0.939,
    "score": 0.939,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "updated_at": "2025-07-19T19:56:12.050287+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa48d"
    },
    "model_id": "deepseek-r1-distill-qwen-32b",
    "analysis_method": "Pass@1",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.051744+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 505,
    "normalized_score": 0.943,
    "score": 0.943,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "updated_at": "2025-07-19T19:56:12.051744+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa492"
    },
    "model_id": "deepseek-v3.1",
    "analysis_method": "Pass@1, Non-Thinking mode",
    "benchmark_id": "hmmt-2025",
    "benchmark_name": "HMMT 2025",
    "created_at": "2025-01-10T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9516,
    "normalized_score": 0.335,
    "score": 0.335,
    "self_reported_source_link": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
    "updated_at": "2025-09-15T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": "Non-thinking: 33.5%, Thinking: 84.2%",
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4a3"
    },
    "model_id": "deepseek-v3-0324",
    "analysis_method": "Exact Match",
    "benchmark_id": "mmlu-pro",
    "benchmark_name": "MMLU-Pro",
    "created_at": "2025-07-19T19:56:11.488686+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 204,
    "normalized_score": 0.812,
    "score": 0.812,
    "self_reported_source_link": "https://api-docs.deepseek.com/news/news250325",
    "updated_at": "2025-07-19T19:56:11.488686+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4a9"
    },
    "model_id": "grok-3-mini",
    "analysis_method": "accuracy",
    "benchmark_id": "livecodebench",
    "benchmark_name": "LiveCodeBench",
    "created_at": "2025-07-19T19:56:13.394024+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1139,
    "normalized_score": 0.804,
    "score": 0.804,
    "self_reported_source_link": "https://x.ai/blog/grok-3",
    "updated_at": "2025-07-19T19:56:13.394024+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4ae"
    },
    "model_id": "grok-4-heavy",
    "analysis_method": "accuracy",
    "benchmark_id": "usamo25",
    "benchmark_name": "USAMO25",
    "created_at": "2025-07-19T19:56:15.070427+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1800,
    "normalized_score": 0.619,
    "score": 0.619,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "updated_at": "2025-07-19T19:56:15.070427+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4b5"
    },
    "model_id": "grok-2-mini",
    "analysis_method": "accuracy",
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "created_at": "2025-07-19T19:56:12.186961+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 577,
    "normalized_score": 0.632,
    "score": 0.632,
    "self_reported_source_link": "https://x.ai/blog/grok-2",
    "updated_at": "2025-07-19T19:56:12.186961+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4be"
    },
    "model_id": "grok-1.5",
    "analysis_method": "0-shot",
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "created_at": "2025-07-19T19:56:12.189264+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 578,
    "normalized_score": 0.536,
    "score": 0.536,
    "self_reported_source_link": "https://x.ai/blog/grok-2",
    "updated_at": "2025-07-19T19:56:12.189264+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4c8"
    },
    "model_id": "grok-4",
    "analysis_method": "accuracy",
    "benchmark_id": "usamo25",
    "benchmark_name": "USAMO25",
    "created_at": "2025-07-19T19:56:15.071894+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1801,
    "normalized_score": 0.375,
    "score": 0.375,
    "self_reported_source_link": "https://x.com/xai/status/1943158495588815072",
    "updated_at": "2025-07-19T19:56:15.071894+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4d0"
    },
    "model_id": "grok-3",
    "analysis_method": "accuracy",
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "created_at": "2025-07-19T19:56:12.191844+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 579,
    "normalized_score": 0.78,
    "score": 0.78,
    "self_reported_source_link": "https://x.ai/blog/grok-3",
    "updated_at": "2025-07-19T19:56:12.191844+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4d6"
    },
    "model_id": "grok-2",
    "analysis_method": "accuracy",
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "created_at": "2025-07-19T19:56:12.193698+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 580,
    "normalized_score": 0.661,
    "score": 0.661,
    "self_reported_source_link": "https://x.ai/blog/grok-2",
    "updated_at": "2025-07-19T19:56:12.193698+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4df"
    },
    "model_id": "grok-1.5v",
    "analysis_method": "zero-shot evaluation",
    "benchmark_id": "textvqa",
    "benchmark_name": "TextVQA",
    "created_at": "2025-07-19T19:56:12.908800+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 915,
    "normalized_score": 0.781,
    "score": 0.781,
    "self_reported_source_link": "https://x.ai/blog/grok-1.5v",
    "updated_at": "2025-07-19T19:56:12.908800+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4e7"
    },
    "model_id": "granite-4.0-tiny-preview",
    "analysis_method": "Score",
    "benchmark_id": "truthfulqa",
    "benchmark_name": "TruthfulQA",
    "created_at": "2025-07-19T19:56:11.358910+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 140,
    "normalized_score": 0.581,
    "score": 0.581,
    "self_reported_source_link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek",
    "updated_at": "2025-07-19T19:56:11.358910+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa4f4"
    },
    "model_id": "granite-3.3-8b-instruct",
    "analysis_method": "Score",
    "benchmark_id": "truthfulqa",
    "benchmark_name": "TruthfulQA",
    "created_at": "2025-07-19T19:56:11.360858+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 141,
    "normalized_score": 0.6686,
    "score": 0.6686,
    "self_reported_source_link": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct",
    "updated_at": "2025-07-19T19:56:11.360858+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa503"
    },
    "model_id": "granite-3.3-8b-base",
    "analysis_method": "Score",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.387990+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 152,
    "normalized_score": 0.744,
    "score": 0.744,
    "self_reported_source_link": "https://huggingface.co/ibm-granite/granite-3.3-8b-base",
    "updated_at": "2025-07-19T19:56:11.387990+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa518"
    },
    "model_id": "glm-4.5-air",
    "analysis_method": "Terminus framework",
    "benchmark_id": "terminal-bench",
    "benchmark_name": "Terminal-Bench",
    "created_at": "2025-07-28T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 7114,
    "normalized_score": 0.3,
    "score": 0.3,
    "self_reported_source_link": "https://z.ai/blog/glm-4.5",
    "updated_at": "2025-07-28T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa527"
    },
    "model_id": "glm-4.5",
    "analysis_method": "Terminus framework",
    "benchmark_id": "terminal-bench",
    "benchmark_name": "Terminal-Bench",
    "created_at": "2025-07-28T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 7014,
    "normalized_score": 0.375,
    "score": 0.375,
    "self_reported_source_link": "https://z.ai/blog/glm-4.5",
    "updated_at": "2025-07-28T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa536"
    },
    "model_id": "llama-3.3-nemotron-super-49b-v1",
    "analysis_method": "Score, Reasoning On",
    "benchmark_id": "mt-bench",
    "benchmark_name": "MT-Bench",
    "created_at": "2025-07-19T19:56:14.527840+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1609,
    "normalized_score": 0.917,
    "score": 0.917,
    "self_reported_source_link": "https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard",
    "updated_at": "2025-07-19T19:56:14.527840+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa53e"
    },
    "model_id": "llama-3.1-nemotron-nano-8b-v1",
    "analysis_method": "Score, Reasoning",
    "benchmark_id": "mt-bench",
    "benchmark_name": "MT-Bench",
    "created_at": "2025-07-19T19:56:14.530016+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1610,
    "normalized_score": 0.81,
    "score": 0.81,
    "self_reported_source_link": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard",
    "updated_at": "2025-07-19T19:56:14.530016+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa546"
    },
    "model_id": "llama-3.1-nemotron-ultra-253b-v1",
    "analysis_method": "Pass@1, Reasoning",
    "benchmark_id": "math-500",
    "benchmark_name": "MATH-500",
    "created_at": "2025-07-19T19:56:12.061892+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 511,
    "normalized_score": 0.97,
    "score": 0.97,
    "self_reported_source_link": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard",
    "updated_at": "2025-07-19T19:56:12.061892+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa54d"
    },
    "model_id": "llama-3.1-nemotron-70b-instruct",
    "analysis_method": "Standard evaluation",
    "benchmark_id": "xlsum-english",
    "benchmark_name": "XLSum English",
    "created_at": "2025-07-19T19:56:15.094560+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1809,
    "normalized_score": 0.3161,
    "score": 0.3161,
    "self_reported_source_link": "https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/",
    "updated_at": "2025-07-19T19:56:15.094560+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa559"
    },
    "model_id": "claude-3-opus-20240229",
    "analysis_method": "0-shot CoT",
    "benchmark_id": "mmlu-pro",
    "benchmark_name": "MMLU-Pro",
    "created_at": "2025-07-19T19:56:11.496438+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 208,
    "normalized_score": 0.685,
    "score": 0.685,
    "self_reported_source_link": "https://arxiv.org/pdf/2406.01574",
    "updated_at": "2025-07-19T19:56:11.496438+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa565"
    },
    "model_id": "claude-opus-4-1-20250805",
    "analysis_method": "Extended thinking (up to 64K tokens). AIME 2025 using nucleus sampling with a top_p of 0.95.",
    "benchmark_id": "aime-2025",
    "benchmark_name": "AIME 2025",
    "created_at": "2025-08-05T00:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 2008,
    "normalized_score": 0.78,
    "score": 0.78,
    "self_reported_source_link": "https://www.anthropic.com/news/claude-opus-4-1",
    "updated_at": "2025-08-05T00:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa56e"
    },
    "model_id": "claude-3-sonnet-20240229",
    "analysis_method": "0-shot CoT",
    "benchmark_id": "mmlu-pro",
    "benchmark_name": "MMLU-Pro",
    "created_at": "2025-07-19T19:56:11.498008+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 209,
    "normalized_score": 0.568,
    "score": 0.568,
    "self_reported_source_link": "https://arxiv.org/pdf/2406.01574",
    "updated_at": "2025-07-19T19:56:11.498008+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa57a"
    },
    "model_id": "claude-3-5-haiku-20241022",
    "analysis_method": "standard",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail",
    "created_at": "2025-07-19T19:56:14.970473+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1757,
    "normalized_score": 0.51,
    "score": 0.51,
    "self_reported_source_link": "https://www.anthropic.com/news/claude-3-5-haiku",
    "updated_at": "2025-07-19T19:56:14.970473+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa584"
    },
    "model_id": "claude-3-7-sonnet-20250219",
    "analysis_method": "Parallel test-time compute, Claude Code agent framework (footnotes 2, 5)",
    "benchmark_id": "terminal-bench",
    "benchmark_name": "Terminal-bench",
    "created_at": "2025-07-19T19:56:12.350298+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 653,
    "normalized_score": 0.352,
    "score": 0.352,
    "self_reported_source_link": "https://www.anthropic.com/news/claude-4",
    "updated_at": "2025-07-19T19:56:12.350298+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa590"
    },
    "model_id": "claude-sonnet-4-20250514",
    "analysis_method": "Parallel test-time compute (multiple attempts, internal scoring model selection). No extended thinking. Claude Code as agent framework. Based on footnotes 2 and 5.",
    "benchmark_id": "terminal-bench",
    "benchmark_name": "Terminal-bench",
    "created_at": "2025-07-19T19:56:12.353338+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 654,
    "normalized_score": 0.355,
    "score": 0.355,
    "self_reported_source_link": "https://www.anthropic.com/news/claude-4",
    "updated_at": "2025-07-19T19:56:12.353338+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa599"
    },
    "model_id": "claude-3-5-sonnet-20241022",
    "analysis_method": "standard",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail",
    "created_at": "2025-07-19T19:56:14.975456+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1760,
    "normalized_score": 0.692,
    "score": 0.692,
    "self_reported_source_link": "https://www.anthropic.com/news/3-5-models-and-computer-use",
    "updated_at": "2025-07-19T19:56:14.975456+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5ad"
    },
    "model_id": "claude-3-haiku-20240307",
    "analysis_method": "5-shot",
    "benchmark_id": "mmlu",
    "benchmark_name": "MMLU",
    "created_at": "2025-07-19T19:56:11.299416+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 106,
    "normalized_score": 0.752,
    "score": 0.752,
    "self_reported_source_link": "https://www.anthropic.com/news/claude-3-haiku",
    "updated_at": "2025-07-19T19:56:11.299416+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5b8"
    },
    "model_id": "claude-3-5-sonnet-20240620",
    "analysis_method": "5-shot",
    "benchmark_id": "mmlu-pro",
    "benchmark_name": "MMLU-Pro",
    "created_at": "2025-07-19T19:56:11.503274+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 212,
    "normalized_score": 0.761,
    "score": 0.761,
    "self_reported_source_link": "https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro",
    "updated_at": "2025-07-19T19:56:11.503274+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5c2"
    },
    "model_id": "claude-opus-4-20250514",
    "analysis_method": "Parallel test-time compute (multiple attempts, internal scoring model selection). No extended thinking. Claude Code as agent framework. Based on footnotes 2 and 5.",
    "benchmark_id": "terminal-bench",
    "benchmark_name": "Terminal-bench",
    "created_at": "2025-07-19T19:56:12.354970+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 655,
    "normalized_score": 0.392,
    "score": 0.392,
    "self_reported_source_link": "https://www.anthropic.com/news/claude-4",
    "updated_at": "2025-07-19T19:56:12.354970+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5cc"
    },
    "model_id": "jamba-1.5-large",
    "analysis_method": "Accuracy",
    "benchmark_id": "wild-bench",
    "benchmark_name": "Wild Bench",
    "created_at": "2025-07-19T19:56:15.125090+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1816,
    "normalized_score": 0.485,
    "score": 0.485,
    "self_reported_source_link": "https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",
    "updated_at": "2025-07-19T19:56:15.125090+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5d5"
    },
    "model_id": "jamba-1.5-mini",
    "analysis_method": "Accuracy",
    "benchmark_id": "wild-bench",
    "benchmark_name": "Wild Bench",
    "created_at": "2025-07-19T19:56:15.127075+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1817,
    "normalized_score": 0.424,
    "score": 0.424,
    "self_reported_source_link": "https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini",
    "updated_at": "2025-07-19T19:56:15.127075+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5de"
    },
    "model_id": "mistral-small-3.1-24b-instruct-2503",
    "analysis_method": "5-shot",
    "benchmark_id": "triviaqa",
    "benchmark_name": "TriviaQA",
    "created_at": "2025-07-19T19:56:11.579482+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 251,
    "normalized_score": 0.805,
    "score": 0.805,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
    "updated_at": "2025-07-19T19:56:11.579482+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5e8"
    },
    "model_id": "mistral-nemo-instruct-2407",
    "analysis_method": "0-shot evaluation",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.392106+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 154,
    "normalized_score": 0.768,
    "score": 0.768,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",
    "updated_at": "2025-07-19T19:56:11.392106+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5f1"
    },
    "model_id": "magistral-small-2506",
    "analysis_method": "v5",
    "benchmark_id": "livecodebench",
    "benchmark_name": "LiveCodeBench",
    "created_at": "2025-07-19T19:56:13.406640+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1144,
    "normalized_score": 0.513,
    "score": 0.513,
    "self_reported_source_link": "https://mistral.ai/news/codestral/",
    "updated_at": "2025-07-19T19:56:13.406640+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5f6"
    },
    "model_id": "magistral-medium",
    "analysis_method": "v6",
    "benchmark_id": "livecodebench",
    "benchmark_name": "LiveCodeBench",
    "created_at": "2025-07-19T19:56:13.408465+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1145,
    "normalized_score": 0.503,
    "score": 0.503,
    "self_reported_source_link": "https://arxiv.org/pdf/2506.10910",
    "updated_at": "2025-07-19T19:56:13.410002+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5fd"
    },
    "model_id": "devstral-medium-2507",
    "analysis_method": "N/A",
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "created_at": "2025-07-19T19:56:13.845635+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1352,
    "normalized_score": 0.616,
    "score": 0.616,
    "self_reported_source_link": "https://mistral.ai/news/devstral-2507",
    "updated_at": "2025-07-19T19:56:13.845635+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa5ff"
    },
    "model_id": "pixtral-large",
    "analysis_method": "VQA Match",
    "benchmark_id": "vqav2",
    "benchmark_name": "VQAv2",
    "created_at": "2025-07-19T19:56:14.414450+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1574,
    "normalized_score": 0.809,
    "score": 0.809,
    "self_reported_source_link": "https://mistral.ai/news/pixtral-large/",
    "updated_at": "2025-07-19T19:56:14.414450+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa607"
    },
    "model_id": "ministral-8b-instruct-2410",
    "analysis_method": "-",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.394106+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 155,
    "normalized_score": 0.753,
    "score": 0.753,
    "self_reported_source_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
    "updated_at": "2025-07-19T19:56:11.394106+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa613"
    },
    "model_id": "mistral-small-24b-instruct-2501",
    "analysis_method": "Score",
    "benchmark_id": "wild-bench",
    "benchmark_name": "Wild Bench",
    "created_at": "2025-07-19T19:56:15.128734+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1818,
    "normalized_score": 0.522,
    "score": 0.522,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
    "updated_at": "2025-07-19T19:56:15.128734+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa61c"
    },
    "model_id": "mistral-small-24b-base-2501",
    "analysis_method": "5-shot",
    "benchmark_id": "triviaqa",
    "benchmark_name": "TriviaQA",
    "created_at": "2025-07-19T19:56:11.585944+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 254,
    "normalized_score": 0.8032,
    "score": 0.8032,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
    "updated_at": "2025-07-19T19:56:11.585944+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa626"
    },
    "model_id": "mistral-small-3.1-24b-base-2503",
    "analysis_method": "5-shot",
    "benchmark_id": "triviaqa",
    "benchmark_name": "TriviaQA",
    "created_at": "2025-07-19T19:56:11.587622+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 255,
    "normalized_score": 0.805,
    "score": 0.805,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
    "updated_at": "2025-07-19T19:56:11.587622+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa62c"
    },
    "model_id": "devstral-small-2507",
    "analysis_method": "OpenHands scaffold",
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "created_at": "2025-07-19T19:56:13.847228+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1353,
    "normalized_score": 0.536,
    "score": 0.536,
    "self_reported_source_link": "https://huggingface.co/mistralai/Devstral-Small-2507",
    "updated_at": "2025-07-19T19:56:13.847228+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa62e"
    },
    "model_id": "pixtral-12b-2409",
    "analysis_method": "VQA Match",
    "benchmark_id": "vqav2",
    "benchmark_name": "VQAv2",
    "created_at": "2025-07-19T19:56:14.416120+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1575,
    "normalized_score": 0.786,
    "score": 0.786,
    "self_reported_source_link": "https://mistral.ai/news/pixtral-12b/",
    "updated_at": "2025-07-19T19:56:14.416120+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa63b"
    },
    "model_id": "codestral-22b",
    "analysis_method": "pass@1",
    "benchmark_id": "spider",
    "benchmark_name": "Spider",
    "created_at": "2025-07-19T19:56:15.159626+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1825,
    "normalized_score": 0.635,
    "score": 0.635,
    "self_reported_source_link": "https://mistral.ai/news/codestral/",
    "updated_at": "2025-07-19T19:56:15.159626+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa643"
    },
    "model_id": "mistral-small-3.2-24b-instruct-2506",
    "analysis_method": "v2",
    "benchmark_id": "wild-bench",
    "benchmark_name": "Wild Bench",
    "created_at": "2025-08-03T22:06:15.130665+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 16782,
    "normalized_score": 0.6533,
    "score": 0.6533,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
    "updated_at": "2025-08-03T22:06:15.130665+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa654"
    },
    "model_id": "mistral-large-2-2407",
    "analysis_method": "Score",
    "benchmark_id": "mt-bench",
    "benchmark_name": "MT-Bench",
    "created_at": "2025-07-19T19:56:14.541051+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1615,
    "normalized_score": 0.863,
    "score": 0.863,
    "self_reported_source_link": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
    "updated_at": "2025-07-19T19:56:14.541051+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa65a"
    },
    "model_id": "gpt-oss-120b",
    "analysis_method": "Function calling",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail benchmark",
    "created_at": "2025-08-05T19:49:05.852855+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 22226,
    "normalized_score": 0.678,
    "score": 0.678,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-oss/",
    "updated_at": "2025-08-05T19:49:05.852855+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa664"
    },
    "model_id": "o3-2025-04-16",
    "analysis_method": "accuracy (avg Airline/Retail)",
    "benchmark_id": "tau-bench",
    "benchmark_name": "Tau-bench",
    "created_at": "2025-07-19T19:56:15.221470+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1844,
    "normalized_score": 0.63,
    "score": 0.63,
    "self_reported_source_link": "https://openai.com/index/introducing-o3-and-o4-mini/",
    "updated_at": "2025-07-19T19:56:15.221470+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa67e"
    },
    "model_id": "gpt-oss-20b",
    "analysis_method": "Function calling",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail benchmark",
    "created_at": "2025-08-05T19:49:05.852855+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 22226,
    "normalized_score": 0.548,
    "score": 0.548,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-oss/",
    "updated_at": "2025-08-05T19:49:05.852855+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa688"
    },
    "model_id": "gpt-4.1-mini-2025-04-14",
    "analysis_method": "GPT-4.1 mini with no tools - Harvard-MIT Mathematics Tournament.",
    "benchmark_id": "hmmt-2025",
    "benchmark_name": "HMMT 2025",
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 10016,
    "normalized_score": 0.35,
    "score": 0.35,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa6a6"
    },
    "model_id": "gpt-5-2025-08-07",
    "analysis_method": "Thinking mode enabled for factual accuracy assessment. Measured hallucination rate on open-source prompts.",
    "benchmark_id": "factscore",
    "benchmark_name": "FactScore",
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 10071,
    "normalized_score": 0.01,
    "score": 0.01,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa6ca"
    },
    "model_id": "o4-mini",
    "analysis_method": "accuracy (o4-mini-high)",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail",
    "created_at": "2025-07-19T19:56:14.980200+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1763,
    "normalized_score": 0.718,
    "score": 0.718,
    "self_reported_source_link": "https://openai.com/index/introducing-o3-and-o4-mini/",
    "updated_at": "2025-07-19T19:56:14.980200+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa6d9"
    },
    "model_id": "gpt-4.1-nano-2025-04-14",
    "analysis_method": "Avg 5 runs, no custom tools/prompting (footnote [4], GPT-4o user model)",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail",
    "created_at": "2025-07-19T19:56:14.982239+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1764,
    "normalized_score": 0.226,
    "score": 0.226,
    "self_reported_source_link": "https://openai.com/index/gpt-4-1/",
    "updated_at": "2025-07-19T19:56:14.982239+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa6f3"
    },
    "model_id": "o3-mini",
    "analysis_method": "benchmark score",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail",
    "created_at": "2025-07-19T19:56:14.984653+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1765,
    "normalized_score": 0.576,
    "score": 0.576,
    "self_reported_source_link": "https://openai.com/index/gpt-4-1/",
    "updated_at": "2025-07-19T19:56:14.984653+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa70e"
    },
    "model_id": "gpt-4o-2024-05-13",
    "analysis_method": "0-shot CoT",
    "benchmark_id": "mmlu-pro",
    "benchmark_name": "MMLU-Pro",
    "created_at": "2025-07-19T19:56:11.515262+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 219,
    "normalized_score": 0.726,
    "score": 0.726,
    "self_reported_source_link": "https://openai.com/blog/gpt-4o",
    "updated_at": "2025-07-19T19:56:11.515262+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa717"
    },
    "model_id": "gpt-4.1-2025-04-14",
    "analysis_method": "GPT-4.1 with no tools - Harvard-MIT Mathematics Tournament.",
    "benchmark_id": "hmmt-2025",
    "benchmark_name": "HMMT 2025",
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 10013,
    "normalized_score": 0.289,
    "score": 0.289,
    "self_reported_source_link": "https://openai.com/index/introducing-gpt-5-for-developers/",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa736"
    },
    "model_id": "o1-pro",
    "analysis_method": "Diamond, Pass@1 accuracy",
    "benchmark_id": "gpqa",
    "benchmark_name": "GPQA",
    "created_at": "2025-07-19T19:56:11.762804+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 354,
    "normalized_score": 0.79,
    "score": 0.79,
    "self_reported_source_link": "https://openai.com/index/introducing-chatgpt-pro/",
    "updated_at": "2025-07-19T19:56:11.762804+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa739"
    },
    "model_id": "gpt-4o-2024-08-06",
    "analysis_method": "GPT-4o without thinking mode - Multi-turn instruction following benchmark.",
    "benchmark_id": "scale-multichallenge",
    "benchmark_name": "Scale MultiChallenge",
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 2005,
    "normalized_score": 0.403,
    "score": 0.403,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa761"
    },
    "model_id": "o1-mini",
    "analysis_method": "Evaluation on validation set",
    "benchmark_id": "superglue",
    "benchmark_name": "SuperGLUE",
    "created_at": "2025-07-19T19:56:15.385801+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1909,
    "normalized_score": 0.75,
    "score": 0.75,
    "self_reported_source_link": "https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/",
    "updated_at": "2025-07-19T19:56:15.385801+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa768"
    },
    "model_id": "gpt-5-nano-2025-08-07",
    "analysis_method": "GPT-5 nano with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.",
    "benchmark_id": "hmmt-2025",
    "benchmark_name": "HMMT 2025",
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9030,
    "normalized_score": 0.756,
    "score": 0.756,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa76e"
    },
    "model_id": "gpt-4.5",
    "analysis_method": "Accuracy",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail",
    "created_at": "2025-07-19T19:56:14.989887+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1768,
    "normalized_score": 0.684,
    "score": 0.684,
    "self_reported_source_link": "https://openai.com/index/gpt-4-1/",
    "updated_at": "2025-07-19T19:56:14.989887+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa78a"
    },
    "model_id": "gpt-5-mini-2025-08-07",
    "analysis_method": "GPT-5 mini with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.",
    "benchmark_id": "hmmt-2025",
    "benchmark_name": "HMMT 2025",
    "created_at": "2025-07-24T12:00:00.000000+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 9029,
    "normalized_score": 0.878,
    "score": 0.878,
    "self_reported_source_link": "https://openai.com/index/gpt-5/",
    "updated_at": "2025-07-24T12:00:00.000000+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa790"
    },
    "model_id": "o1-2024-12-17",
    "analysis_method": "agents",
    "benchmark_id": "tau-bench-retail",
    "benchmark_name": "TAU-bench Retail",
    "created_at": "2025-07-19T19:56:14.992114+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1769,
    "normalized_score": 0.708,
    "score": 0.708,
    "self_reported_source_link": "https://openai.com/index/gpt-4-1/",
    "updated_at": "2025-07-19T19:56:14.992114+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa7a4"
    },
    "model_id": "gpt-3.5-turbo-0125",
    "analysis_method": "Accuracy",
    "benchmark_id": "mmmu",
    "benchmark_name": "MMMU",
    "created_at": "2025-07-19T19:56:12.230222+00:00",
    "is_self_reported": false,
    "model_benchmark_id": 597,
    "normalized_score": 0.0,
    "score": 0.0,
    "self_reported_source_link": "https://example.com/benchmark-image",
    "updated_at": "2025-07-19T19:56:12.230222+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa7ad"
    },
    "model_id": "o1-preview",
    "analysis_method": "Verified",
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "created_at": "2025-07-19T19:56:13.867753+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1362,
    "normalized_score": 0.413,
    "score": 0.413,
    "self_reported_source_link": "https://openai.com/index/learning-to-reason-with-llms/",
    "updated_at": "2025-07-19T19:56:13.867753+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa7b6"
    },
    "model_id": "gpt-4o-mini-2024-07-18",
    "analysis_method": "Pass Rate",
    "benchmark_id": "swe-bench-verified",
    "benchmark_name": "SWE-Bench Verified",
    "created_at": "2025-07-19T19:56:13.870038+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 1363,
    "normalized_score": 0.087,
    "score": 0.087,
    "self_reported_source_link": "https://openai.com/index/gpt-4-1/",
    "updated_at": "2025-07-19T19:56:13.870038+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa7c0"
    },
    "model_id": "gpt-4-0613",
    "analysis_method": "5-shot, Commonsense reasoning around pronoun resolution",
    "benchmark_id": "winogrande",
    "benchmark_name": "Winogrande",
    "created_at": "2025-07-19T19:56:11.396099+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 156,
    "normalized_score": 0.875,
    "score": 0.875,
    "self_reported_source_link": "https://openai.com/research/gpt-4",
    "updated_at": "2025-07-19T19:56:11.396099+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  },
  {
    "_id": {
      "$oid": "692a9088a6d7100bb5daa7cd"
    },
    "model_id": "gpt-4-turbo-2024-04-09",
    "analysis_method": "Multiple-choice questions in 57 subjects (professional & academic)",
    "benchmark_id": "mmlu",
    "benchmark_name": "MMLU",
    "created_at": "2025-07-19T19:56:11.337995+00:00",
    "is_self_reported": true,
    "model_benchmark_id": 130,
    "normalized_score": 0.865,
    "score": 0.865,
    "self_reported_source_link": "https://openai.com/index/hello-gpt-4o/",
    "updated_at": "2025-07-19T19:56:11.337995+00:00",
    "verification_date": null,
    "verification_hardware": null,
    "verification_notes": null,
    "verification_provider_id": null,
    "verified_by_llmstats": false
  }
]