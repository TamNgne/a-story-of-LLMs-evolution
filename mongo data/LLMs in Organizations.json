[{
  "_id": {
    "$oid": "69255674b7cff6f0e90665d6"
  },
  "model_id": "jamba-1.5-large",
  "announcement_date": "2024-08-22",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.764734+00:00",
  "description": "State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-03-05",
  "license_id": "jamba_open_model_license",
  "model_family_id": null,
  "model_name": "jamba-1.5-large",
  "multimodal": false,
  "name": "Jamba 1.5 Large",
  "organization": "ai21",
  "organization_id": "ai21",
  "param_count": {
    "$numberLong": "398000000000"
  },
  "release_date": "2024-08-22",
  "source_api_ref": "https://docs.ai21.com/reference/jamba-15-api-ref",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.ai21.com/blog/announcing-jamba-model-family",
  "source_weights_link": "https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.764734+00:00",
  "avg_benchmark_score": 0.65475
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665d7"
  },
  "model_id": "jamba-1.5-mini",
  "announcement_date": "2024-08-22",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.767535+00:00",
  "description": "Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-03-05",
  "license_id": "jamba_open_model_license",
  "model_family_id": null,
  "model_name": "jamba-1.5-mini",
  "multimodal": false,
  "name": "Jamba 1.5 Mini",
  "organization": "ai21",
  "organization_id": "ai21",
  "param_count": {
    "$numberLong": "52000000000"
  },
  "release_date": "2024-08-22",
  "source_api_ref": "https://docs.ai21.com/reference/jamba-15-api-ref",
  "source_paper": "https://arxiv.org/abs/2408.12570",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.ai21.com/blog/announcing-jamba-model-family",
  "source_weights_link": "https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.767535+00:00",
  "avg_benchmark_score": 0.56075
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665d8"
  },
  "model_id": "nova-lite",
  "announcement_date": "2024-11-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.429271+00:00",
  "description": "A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "nova-lite",
  "multimodal": true,
  "name": "Nova Lite",
  "organization": "amazon",
  "organization_id": "amazon",
  "param_count": null,
  "release_date": "2024-11-20",
  "source_api_ref": "https://aws.amazon.com/bedrock/amazon-nova-lite",
  "source_paper": "https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.429271+00:00",
  "avg_benchmark_score": 0.7072592592592593
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665d9"
  },
  "model_id": "nova-micro",
  "announcement_date": "2024-11-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.435386+00:00",
  "description": "A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "nova-micro",
  "multimodal": false,
  "name": "Nova Micro",
  "organization": "amazon",
  "organization_id": "amazon",
  "param_count": null,
  "release_date": "2024-11-20",
  "source_api_ref": "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-nova.html",
  "source_paper": "https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card",
  "source_playground": null,
  "source_repo_link": "https://huggingface.co/amazon-agi",
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.435386+00:00",
  "avg_benchmark_score": 0.6704705882352942
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665da"
  },
  "model_id": "nova-pro",
  "announcement_date": "2024-11-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.431675+00:00",
  "description": "Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "nova-pro",
  "multimodal": true,
  "name": "Nova Pro",
  "organization": "amazon",
  "organization_id": "amazon",
  "param_count": null,
  "release_date": "2024-11-20",
  "source_api_ref": "https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-nova.html",
  "source_paper": "https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card",
  "source_playground": null,
  "source_repo_link": "https://huggingface.co/amazon-agi",
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.431675+00:00",
  "avg_benchmark_score": 0.731925925925926
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665db"
  },
  "model_id": "claude-3-5-haiku-20241022",
  "announcement_date": "2024-10-22",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.744002+00:00",
  "description": "Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-3-5-haiku-20241022",
  "multimodal": false,
  "name": "Claude 3.5 Haiku",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2024-10-22",
  "source_api_ref": "https://docs.anthropic.com/en/docs/intro-to-claude#claude-3-5-family",
  "source_paper": null,
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-3-5-haiku",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.744002+00:00",
  "avg_benchmark_score": 0.608
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665dc"
  },
  "model_id": "claude-3-5-sonnet-20240620",
  "announcement_date": "2024-06-21",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.757926+00:00",
  "description": "Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-3-5-sonnet-20240620",
  "multimodal": true,
  "name": "Claude 3.5 Sonnet",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2024-06-21",
  "source_api_ref": "https://docs.anthropic.com/en/docs/intro-to-claude#claude-3-5-family",
  "source_paper": null,
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-3-5-sonnet",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.757926+00:00",
  "avg_benchmark_score": 0.8413333333333334
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665dd"
  },
  "model_id": "claude-3-5-sonnet-20241022",
  "announcement_date": "2024-10-22",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.752534+00:00",
  "description": "Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-3-5-sonnet-20241022",
  "multimodal": true,
  "name": "Claude 3.5 Sonnet",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2024-10-22",
  "source_api_ref": "https://docs.anthropic.com/en/docs/intro-to-claude#claude-3-5-family",
  "source_paper": "https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf",
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-3-5-sonnet",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.752534+00:00",
  "avg_benchmark_score": 0.7332631578947368
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665de"
  },
  "model_id": "claude-3-7-sonnet-20250219",
  "announcement_date": "2025-02-24",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.747775+00:00",
  "description": "The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-3-7-sonnet-20250219",
  "multimodal": true,
  "name": "Claude 3.7 Sonnet",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2025-02-24",
  "source_api_ref": "https://docs.anthropic.com/en/docs/about-claude/models/all-models",
  "source_paper": null,
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-3-7-sonnet",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.747775+00:00",
  "avg_benchmark_score": 0.741090909090909
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665df"
  },
  "model_id": "claude-3-haiku-20240307",
  "announcement_date": "2024-03-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.755159+00:00",
  "description": "Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-3-haiku-20240307",
  "multimodal": true,
  "name": "Claude 3 Haiku",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2024-03-13",
  "source_api_ref": "https://www.anthropic.com/claude",
  "source_paper": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-3-haiku",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.755159+00:00",
  "avg_benchmark_score": 0.7144999999999999
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e0"
  },
  "model_id": "claude-3-opus-20240229",
  "announcement_date": "2024-02-29",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.738279+00:00",
  "description": "Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-3-opus-20240229",
  "multimodal": true,
  "name": "Claude 3 Opus",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2024-02-29",
  "source_api_ref": "https://www.anthropic.com/claude",
  "source_paper": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-3-family",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.738279+00:00",
  "avg_benchmark_score": 0.8164545454545454
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e1"
  },
  "model_id": "claude-3-sonnet-20240229",
  "announcement_date": "2024-02-29",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.740647+00:00",
  "description": "Claude 3 Sonnet strikes the ideal balance between intelligence and speedâ€”particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-3-sonnet-20240229",
  "multimodal": true,
  "name": "Claude 3 Sonnet",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2024-02-29",
  "source_api_ref": "https://www.anthropic.com/claude",
  "source_paper": "https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf",
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-3-family",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.740647+00:00",
  "avg_benchmark_score": 0.7382727272727273
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e2"
  },
  "model_id": "claude-opus-4-1-20250805",
  "announcement_date": "2025-08-05",
  "available_in_zeroeval": true,
  "created_at": "2025-08-05T00:00:00.000000+00:00",
  "description": "Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-opus-4-1-20250805",
  "multimodal": true,
  "name": "Claude Opus 4.1",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2025-08-05",
  "source_api_ref": "https://docs.anthropic.com/en/docs/about-claude/models/all-models",
  "source_paper": null,
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-opus-4-1",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-08-05T00:00:00.000000+00:00",
  "avg_benchmark_score": null
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e3"
  },
  "model_id": "claude-opus-4-20250514",
  "announcement_date": "2025-05-22",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.760983+00:00",
  "description": "Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-opus-4-20250514",
  "multimodal": true,
  "name": "Claude Opus 4",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2025-05-22",
  "source_api_ref": "https://docs.anthropic.com/en/docs/about-claude/models/all-models",
  "source_paper": null,
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-4",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.760983+00:00",
  "avg_benchmark_score": 0.6463333333333333
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e4"
  },
  "model_id": "claude-sonnet-4-20250514",
  "announcement_date": "2025-05-22",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.750182+00:00",
  "description": "Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "claude-sonnet-4-20250514",
  "multimodal": true,
  "name": "Claude Sonnet 4",
  "organization": "anthropic",
  "organization_id": "anthropic",
  "param_count": null,
  "release_date": "2025-05-22",
  "source_api_ref": "https://docs.anthropic.com/en/docs/about-claude/models/all-models",
  "source_paper": null,
  "source_playground": "https://claude.ai",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.anthropic.com/news/claude-4",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.750182+00:00",
  "avg_benchmark_score": 0.694375
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e5"
  },
  "model_id": "command-r-plus-04-2024",
  "announcement_date": "2024-08-30",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.415748+00:00",
  "description": "C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "cc_by_nc",
  "model_family_id": null,
  "model_name": "command-r-plus-04-2024",
  "multimodal": false,
  "name": "Command R+",
  "organization": "cohere",
  "organization_id": "cohere",
  "param_count": {
    "$numberLong": "104000000000"
  },
  "release_date": "2024-08-30",
  "source_api_ref": "https://docs.cohere.com/v2/docs/command-r-plus",
  "source_paper": null,
  "source_playground": "https://huggingface.co/CohereForAI/c4ai-command-r-plus",
  "source_repo_link": "https://huggingface.co/CohereForAI/c4ai-command-r-plus",
  "source_scorecard_blog_link": null,
  "source_weights_link": "",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.415748+00:00",
  "avg_benchmark_score": 0.74615
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e6"
  },
  "model_id": "deepseek-r1",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": false,
  "created_at": "2025-01-20T00:00:00.000000+00:00",
  "description": "DeepSeek-R1 is a reasoning-focused language model from DeepSeek that features advanced thinking capabilities. It serves as the foundation for DeepSeek's reasoning model family and pioneered their thinking mode approach for complex problem-solving tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1",
  "multimodal": false,
  "name": "DeepSeek-R1",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "671000000000"
  },
  "release_date": "2025-01-20",
  "source_api_ref": "https://api.deepseek.com/docs",
  "source_paper": null,
  "source_playground": "https://chat.deepseek.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.deepseek.com/",
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
  "training_tokens": null,
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": null
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e7"
  },
  "model_id": "deepseek-r1-0528",
  "announcement_date": "2025-05-28",
  "available_in_zeroeval": false,
  "created_at": "2025-05-28T00:00:00.000000+00:00",
  "description": "DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",
  "fine_tuned_from_model_id": "deepseek-r1",
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-0528",
  "multimodal": false,
  "name": "DeepSeek-R1-0528",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "671000000000"
  },
  "release_date": "2025-05-28",
  "source_api_ref": "https://api.deepseek.com/docs",
  "source_paper": null,
  "source_playground": "https://chat.deepseek.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.deepseek.com/",
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
  "training_tokens": null,
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 0.60145625
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e8"
  },
  "model_id": "deepseek-r1-distill-llama-70b",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.685839+00:00",
  "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-distill-llama-70b",
  "multimodal": false,
  "name": "DeepSeek R1 Distill Llama 70B",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "70600000000"
  },
  "release_date": "2025-01-20",
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_paper": "https://arxiv.org/pdf/2501.12948",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.685839+00:00",
  "avg_benchmark_score": 0.7597499999999999
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665e9"
  },
  "model_id": "deepseek-r1-distill-llama-8b",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.683265+00:00",
  "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-distill-llama-8b",
  "multimodal": false,
  "name": "DeepSeek R1 Distill Llama 8B",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "8030000000"
  },
  "release_date": "2025-01-20",
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_paper": "https://arxiv.org/pdf/2501.12948",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.683265+00:00",
  "avg_benchmark_score": 0.64425
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665ea"
  },
  "model_id": "deepseek-r1-distill-qwen-1.5b",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.672853+00:00",
  "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-distill-qwen-1.5b",
  "multimodal": false,
  "name": "DeepSeek R1 Distill Qwen 1.5B",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": 1780000000,
  "release_date": "2025-01-20",
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_paper": "https://arxiv.org/pdf/2501.12948",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.672853+00:00",
  "avg_benchmark_score": 0.46825
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665eb"
  },
  "model_id": "deepseek-r1-distill-qwen-14b",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.688267+00:00",
  "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-distill-qwen-14b",
  "multimodal": false,
  "name": "DeepSeek R1 Distill Qwen 14B",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "14800000000"
  },
  "release_date": "2025-01-20",
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_paper": "https://arxiv.org/pdf/2501.12948",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.688267+00:00",
  "avg_benchmark_score": 0.7152499999999999
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665ec"
  },
  "model_id": "deepseek-r1-distill-qwen-32b",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.690560+00:00",
  "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-distill-qwen-32b",
  "multimodal": false,
  "name": "DeepSeek R1 Distill Qwen 32B",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "32800000000"
  },
  "release_date": "2025-01-20",
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_paper": "https://arxiv.org/pdf/2501.12948",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.690560+00:00",
  "avg_benchmark_score": 0.74225
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665ed"
  },
  "model_id": "deepseek-r1-distill-qwen-7b",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.669926+00:00",
  "description": "DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-distill-qwen-7b",
  "multimodal": false,
  "name": "DeepSeek R1 Distill Qwen 7B",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "7620000000"
  },
  "release_date": "2025-01-20",
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_paper": "https://arxiv.org/pdf/2501.12948",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.669926+00:00",
  "avg_benchmark_score": 0.657
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665ee"
  },
  "model_id": "deepseek-r1-zero",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.902496+00:00",
  "description": "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",
  "fine_tuned_from_model_id": "deepseek-v3",
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-r1-zero",
  "multimodal": false,
  "name": "DeepSeek R1 Zero",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "671000000000"
  },
  "release_date": "2025-01-20",
  "source_api_ref": "https://api-docs.deepseek.com/news/news250120",
  "source_paper": "https://arxiv.org/abs/2501.12948",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-R1",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.902496+00:00",
  "avg_benchmark_score": 0.76475
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665ef"
  },
  "model_id": "deepseek-v2.5",
  "announcement_date": "2024-05-08",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.680851+00:00",
  "description": "DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "deepseek",
  "model_family_id": null,
  "model_name": "deepseek-v2.5",
  "multimodal": false,
  "name": "DeepSeek-V2.5",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "236000000000"
  },
  "release_date": "2024-05-08",
  "source_api_ref": "https://www.deepseek.com/",
  "source_paper": "https://arxiv.org/abs/2405.04434",
  "source_playground": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
  "source_repo_link": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-V2.5",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.680851+00:00",
  "avg_benchmark_score": 0.7111999999999999
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f0"
  },
  "model_id": "deepseek-v3",
  "announcement_date": "2024-12-25",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.677307+00:00",
  "description": "A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit_+_model_license_(commercial_use_allowed)",
  "model_family_id": null,
  "model_name": "deepseek-v3",
  "multimodal": false,
  "name": "DeepSeek-V3",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "671000000000"
  },
  "release_date": "2024-12-25",
  "source_api_ref": "https://platform.deepseek.com",
  "source_paper": "https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-V3",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.677307+00:00",
  "avg_benchmark_score": 0.6717500000000001
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f1"
  },
  "model_id": "deepseek-v3-0324",
  "announcement_date": "2025-03-25",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.693499+00:00",
  "description": "A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit_+_model_license_(commercial_use_allowed)",
  "model_family_id": null,
  "model_name": "deepseek-v3-0324",
  "multimodal": false,
  "name": "DeepSeek-V3 0324",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "671000000000"
  },
  "release_date": "2025-03-25",
  "source_api_ref": "https://platform.deepseek.com",
  "source_paper": "https://arxiv.org/abs/2412.19437",
  "source_playground": "https://chat.deepseek.com",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-V3",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
  "training_tokens": {
    "$numberLong": "14800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.693499+00:00",
  "avg_benchmark_score": 0.7044
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f2"
  },
  "model_id": "deepseek-v3.1",
  "announcement_date": "2025-01-10",
  "available_in_zeroeval": true,
  "created_at": "2025-01-10T00:00:00.000000+00:00",
  "description": "DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",
  "fine_tuned_from_model_id": "deepseek-v3",
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "deepseek-v3.1",
  "multimodal": false,
  "name": "DeepSeek-V3.1",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "671000000000"
  },
  "release_date": "2025-01-10",
  "source_api_ref": "https://api.deepseek.com/docs",
  "source_paper": "https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek-V3.pdf",
  "source_playground": "https://chat.deepseek.com/",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-V3",
  "source_scorecard_blog_link": "https://www.deepseek.com/news/deepseek-v3-1",
  "source_weights_link": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
  "training_tokens": null,
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 0.58425
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f3"
  },
  "model_id": "deepseek-vl2",
  "announcement_date": "2024-12-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.658016+00:00",
  "description": "An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "deepseek",
  "model_family_id": null,
  "model_name": "deepseek-vl2",
  "multimodal": true,
  "name": "DeepSeek VL2",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "27000000000"
  },
  "release_date": "2024-12-13",
  "source_api_ref": "https://www.deepseek.com/",
  "source_paper": "https://arxiv.org/pdf/2412.10302",
  "source_playground": "https://huggingface.co/deepseek-ai/deepseek-vl2",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-VL2?tab=readme-ov-file",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/deepseek-vl2",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.658016+00:00",
  "avg_benchmark_score": 0.7090214285714286
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f4"
  },
  "model_id": "deepseek-vl2-small",
  "announcement_date": "2024-12-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.666424+00:00",
  "description": "An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "deepseek",
  "model_family_id": null,
  "model_name": "deepseek-vl2-small",
  "multimodal": true,
  "name": "DeepSeek VL2 Small",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "16000000000"
  },
  "release_date": "2024-12-13",
  "source_api_ref": "https://www.deepseek.com/",
  "source_paper": "https://arxiv.org/pdf/2412.10302",
  "source_playground": "https://huggingface.co/deepseek-ai/deepseek-vl2-small",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-VL2",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/deepseek-vl2-small",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.666424+00:00",
  "avg_benchmark_score": 0.6958785714285715
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f5"
  },
  "model_id": "deepseek-vl2-tiny",
  "announcement_date": "2024-12-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.662552+00:00",
  "description": "An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "deepseek",
  "model_family_id": null,
  "model_name": "deepseek-vl2-tiny",
  "multimodal": true,
  "name": "DeepSeek VL2 Tiny",
  "organization": "deepseek",
  "organization_id": "deepseek",
  "param_count": {
    "$numberLong": "3000000000"
  },
  "release_date": "2024-12-13",
  "source_api_ref": "https://www.deepseek.com/",
  "source_paper": "https://arxiv.org/pdf/2412.10302",
  "source_playground": "https://huggingface.co/deepseek-ai/deepseek-vl2-tiny",
  "source_repo_link": "https://github.com/deepseek-ai/DeepSeek-VL2",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/deepseek-ai/deepseek-vl2-tiny",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.662552+00:00",
  "avg_benchmark_score": 0.6310357142857143
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f6"
  },
  "model_id": "gemini-1.0-pro",
  "announcement_date": "2024-02-15",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.461784+00:00",
  "description": "Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-02-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-1.0-pro",
  "multimodal": false,
  "name": "Gemini 1.0 Pro",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2024-02-15",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.0-pro",
  "source_paper": "https://arxiv.org/pdf/2312.11805",
  "source_playground": "https://gemini.google/advanced/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://blog.google/technology/ai/google-gemini-ai/#scalable-efficient",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.461784+00:00",
  "avg_benchmark_score": 0.484
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f7"
  },
  "model_id": "gemini-1.5-flash",
  "announcement_date": "2024-05-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.514569+00:00",
  "description": "Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-11-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-1.5-flash",
  "multimodal": true,
  "name": "Gemini 1.5 Flash",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2024-05-01",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash",
  "source_paper": "https://arxiv.org/pdf/2403.05530",
  "source_playground": "https://ai.google.dev/studio",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://deepmind.google/technologies/gemini/flash/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.514569+00:00",
  "avg_benchmark_score": 0.667590909090909
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f8"
  },
  "model_id": "gemini-1.5-flash-8b",
  "announcement_date": "2024-03-15",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.530672+00:00",
  "description": "A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-10-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-1.5-flash-8b",
  "multimodal": true,
  "name": "Gemini 1.5 Flash 8B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2024-03-15",
  "source_api_ref": "https://ai.google.dev/docs/gemini_1.5_flash",
  "source_paper": null,
  "source_playground": "https://ai.google.dev/studio",
  "source_repo_link": "https://github.com/google/generative-ai",
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.530672+00:00",
  "avg_benchmark_score": 0.6045384615384616
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665f9"
  },
  "model_id": "gemini-1.5-pro",
  "announcement_date": "2024-05-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.481673+00:00",
  "description": "Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-11-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-1.5-pro",
  "multimodal": true,
  "name": "Gemini 1.5 Pro",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2024-05-01",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-pro",
  "source_paper": "https://arxiv.org/pdf/2403.05530",
  "source_playground": "https://ai.google.dev/studio",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://deepmind.google/technologies/gemini/pro/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.481673+00:00",
  "avg_benchmark_score": 0.725695652173913
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665fa"
  },
  "model_id": "gemini-2.0-flash",
  "announcement_date": "2024-12-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.538624+00:00",
  "description": "Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-08-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-2.0-flash",
  "multimodal": true,
  "name": "Gemini 2.0 Flash",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2024-12-01",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-flash",
  "source_paper": null,
  "source_playground": "https://ai.google.dev/studio",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.538624+00:00",
  "avg_benchmark_score": 0.6666153846153846
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665fb"
  },
  "model_id": "gemini-2.0-flash-lite",
  "announcement_date": "2025-02-05",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.469548+00:00",
  "description": "A Gemini 2.0 Flash model optimized for cost efficiency and low latency",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-2.0-flash-lite",
  "multimodal": true,
  "name": "Gemini 2.0 Flash-Lite",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2025-02-05",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash-lite",
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/prompts/new_chat?model=gemini-2.0-flash-lite",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://developers.googleblog.com/en/gemini-2-family-expands",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.469548+00:00",
  "avg_benchmark_score": 0.5896923076923077
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665fc"
  },
  "model_id": "gemini-2.0-flash-thinking",
  "announcement_date": "2025-01-21",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.504495+00:00",
  "description": "Gemini 2.0 Flash Thinking is a enhanced reasoning model, capable of showing its thoughts to improve performance and explainability. Combining speed and performance, Gemini 2.0 Flash Thinking also excels in science and math, showing its thinking to solve complex problems.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-08-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-2.0-flash-thinking",
  "multimodal": true,
  "name": "Gemini 2.0 Flash Thinking",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2025-01-21",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-flash-thinking-experimental",
  "source_paper": null,
  "source_playground": "https://ai.google.dev/studio",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.504495+00:00",
  "avg_benchmark_score": 0.743
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665fd"
  },
  "model_id": "gemini-2.5-flash",
  "announcement_date": "2025-05-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.500918+00:00",
  "description": "A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-01-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-2.5-flash",
  "multimodal": true,
  "name": "Gemini 2.5 Flash",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2025-05-20",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models?hl=en#gemini-2.5-flash-preview-04-17",
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/?model=gemini-2.5-flash-preview-04-17",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://developers.googleblog.com/en/start-building-with-gemini-25-flash/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.500918+00:00",
  "avg_benchmark_score": 0.6245714285714286
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665fe"
  },
  "model_id": "gemini-2.5-flash-lite",
  "announcement_date": "2025-06-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.473471+00:00",
  "description": "Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-01-01",
  "license_id": "creative_commons_attribution_4_0_license",
  "model_family_id": null,
  "model_name": "gemini-2.5-flash-lite",
  "multimodal": true,
  "name": "Gemini 2.5 Flash-Lite",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2025-06-17",
  "source_api_ref": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite",
  "source_paper": "https://arxiv.org/abs/2503.16534",
  "source_playground": "https://ai.google.com/studio",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.473471+00:00",
  "avg_benchmark_score": 0.4082307692307693
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e90665ff"
  },
  "model_id": "gemini-2.5-pro",
  "announcement_date": "2025-05-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.458697+00:00",
  "description": "Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-01-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-2.5-pro",
  "multimodal": true,
  "name": "Gemini 2.5 Pro",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2025-05-20",
  "source_api_ref": "https://ai.google.dev/gemini-api/docs/models?hl=en#gemini-2.5-pro-preview-03-25",
  "source_paper": "https://storage.googleapis.com/model-cards/documents/gemini-2.5-pro-preview.pdf",
  "source_playground": "https://aistudio.google.com/?model=gemini-2.5-pro-preview-03-25",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.458697+00:00",
  "avg_benchmark_score": 0.7088
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066600"
  },
  "model_id": "gemini-2.5-pro-preview-06-05",
  "announcement_date": "2025-06-05",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.493595+00:00",
  "description": "The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-01-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-2.5-pro-preview-06-05",
  "multimodal": true,
  "name": "Gemini 2.5 Pro Preview 06-05",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2025-06-05",
  "source_api_ref": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",
  "source_paper": null,
  "source_playground": "https://aistudio.google.com",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.493595+00:00",
  "avg_benchmark_score": 0.6881538461538461
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066601"
  },
  "model_id": "gemini-diffusion",
  "announcement_date": "2025-05-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.534835+00:00",
  "description": "Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemini-diffusion",
  "multimodal": false,
  "name": "Gemini Diffusion",
  "organization": "google",
  "organization_id": "google",
  "param_count": null,
  "release_date": "2025-05-20",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/google",
  "source_scorecard_blog_link": "https://deepmind.google/models/gemini-diffusion/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.534835+00:00",
  "avg_benchmark_score": 0.4694
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066602"
  },
  "model_id": "gemma-2-27b-it",
  "announcement_date": "2024-06-27",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.485572+00:00",
  "description": "Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-2-27b-it",
  "multimodal": false,
  "name": "Gemma 2 27B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "27200000000"
  },
  "release_date": "2024-06-27",
  "source_api_ref": "https://huggingface.co/google/gemma-2-27b-it",
  "source_paper": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
  "source_playground": "https://huggingface.co/chat/models/google/gemma-2-27b-it",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://huggingface.co/blog/gemma2",
  "source_weights_link": "https://huggingface.co/google/gemma-2-27b-it",
  "training_tokens": {
    "$numberLong": "13000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.485572+00:00",
  "avg_benchmark_score": 0.6911875
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066603"
  },
  "model_id": "gemma-2-9b-it",
  "announcement_date": "2024-06-27",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.477806+00:00",
  "description": "Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-2-9b-it",
  "multimodal": false,
  "name": "Gemma 2 9B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "9240000000"
  },
  "release_date": "2024-06-27",
  "source_api_ref": "https://huggingface.co/google/gemma-2-9b-it",
  "source_paper": "https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf",
  "source_playground": "https://huggingface.co/chat/models/google/gemma-2-9b-it",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://huggingface.co/blog/gemma2",
  "source_weights_link": "https://huggingface.co/google/gemma-2-9b-it",
  "training_tokens": {
    "$numberLong": "8000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.477806+00:00",
  "avg_benchmark_score": 0.6463125000000001
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066604"
  },
  "model_id": "gemma-3-12b-it",
  "announcement_date": "2025-03-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.444134+00:00",
  "description": "Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-3-12b-it",
  "multimodal": true,
  "name": "Gemma 3 12B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "12000000000"
  },
  "release_date": "2025-03-12",
  "source_api_ref": null,
  "source_paper": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://huggingface.co/blog/gemma3",
  "source_weights_link": "https://huggingface.co/google/gemma-3-12b-it",
  "training_tokens": {
    "$numberLong": "12000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.444134+00:00",
  "avg_benchmark_score": 0.64704
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066605"
  },
  "model_id": "gemma-3-1b-it",
  "announcement_date": "2025-03-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.527185+00:00",
  "description": "The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-3-1b-it",
  "multimodal": false,
  "name": "Gemma 3 1B",
  "organization": "google",
  "organization_id": "google",
  "param_count": 1000000000,
  "release_date": "2025-03-12",
  "source_api_ref": "https://huggingface.co/google/gemma-3-1b-it",
  "source_paper": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
  "source_playground": "https://huggingface.co/chat/models/google/gemma-3-1b-it",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://huggingface.co/blog/gemma3",
  "source_weights_link": "https://huggingface.co/google/gemma-3-1b-it",
  "training_tokens": {
    "$numberLong": "2000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.527185+00:00",
  "avg_benchmark_score": 0.29894444444444446
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066606"
  },
  "model_id": "gemma-3-27b-it",
  "announcement_date": "2025-03-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.523800+00:00",
  "description": "Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-3-27b-it",
  "multimodal": true,
  "name": "Gemma 3 27B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "27000000000"
  },
  "release_date": "2025-03-12",
  "source_api_ref": null,
  "source_paper": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://huggingface.co/blog/gemma3",
  "source_weights_link": "https://huggingface.co/google/gemma-3-27b-it",
  "training_tokens": {
    "$numberLong": "14000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.523800+00:00",
  "avg_benchmark_score": 0.6544615384615384
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066607"
  },
  "model_id": "gemma-3-4b-it",
  "announcement_date": "2025-03-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.520515+00:00",
  "description": "Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-08-01",
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-3-4b-it",
  "multimodal": true,
  "name": "Gemma 3 4B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "4000000000"
  },
  "release_date": "2025-03-12",
  "source_api_ref": null,
  "source_paper": "https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://huggingface.co/blog/gemma3",
  "source_weights_link": "https://huggingface.co/google/gemma-3-4b-it",
  "training_tokens": {
    "$numberLong": "4000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.520515+00:00",
  "avg_benchmark_score": 0.5298846153846154
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066608"
  },
  "model_id": "gemma-3n-e2b",
  "announcement_date": "2025-06-26",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.508070+00:00",
  "description": "Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemma-3n-e2b",
  "multimodal": true,
  "name": "Gemma 3n E2B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2025-06-26",
  "source_api_ref": "https://huggingface.co/blog/gemma3n",
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://ai.google.dev/gemma/docs/gemma-3n",
  "source_weights_link": "https://huggingface.co/google/gemma-3n-E2B",
  "training_tokens": {
    "$numberLong": "11000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.508070+00:00",
  "avg_benchmark_score": 0.5864545454545455
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066609"
  },
  "model_id": "gemma-3n-e2b-it",
  "announcement_date": "2025-06-26",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.541972+00:00",
  "description": "Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemma-3n-e2b-it",
  "multimodal": true,
  "name": "Gemma 3n E2B Instructed",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2025-06-26",
  "source_api_ref": "https://huggingface.co/blog/gemma3n",
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://ai.google.dev/gemma/docs/gemma-3n",
  "source_weights_link": "https://huggingface.co/google/gemma-3n-E2B-it",
  "training_tokens": {
    "$numberLong": "11000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.541972+00:00",
  "avg_benchmark_score": 0.3372777777777778
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906660a"
  },
  "model_id": "gemma-3n-e2b-it-litert-preview",
  "announcement_date": "2025-05-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.466473+00:00",
  "description": "Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-3n-e2b-it-litert-preview",
  "multimodal": true,
  "name": "Gemma 3n E2B Instructed LiteRT (Preview)",
  "organization": "google",
  "organization_id": "google",
  "param_count": 1910000000,
  "release_date": "2025-05-20",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/",
  "source_repo_link": "https://huggingface.co/google/gemma-3n-E2B-it-litert-preview",
  "source_scorecard_blog_link": "https://ai.google.dev/gemma/docs/gemma-3n",
  "source_weights_link": "https://huggingface.co/google/gemma-3n-E2B-it-litert-preview",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.466473+00:00",
  "avg_benchmark_score": 0.43925
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906660b"
  },
  "model_id": "gemma-3n-e4b",
  "announcement_date": "2025-06-26",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.440084+00:00",
  "description": "Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemma-3n-e4b",
  "multimodal": true,
  "name": "Gemma 3n E4B",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2025-06-26",
  "source_api_ref": "https://huggingface.co/blog/gemma3n",
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://ai.google.dev/gemma/docs/gemma-3n",
  "source_weights_link": "https://huggingface.co/google/gemma-3n-E4B",
  "training_tokens": {
    "$numberLong": "11000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.440084+00:00",
  "avg_benchmark_score": 0.6462727272727272
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906660c"
  },
  "model_id": "gemma-3n-e4b-it",
  "announcement_date": "2025-06-26",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.517334+00:00",
  "description": "Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gemma-3n-e4b-it",
  "multimodal": true,
  "name": "Gemma 3n E4B Instructed",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2025-06-26",
  "source_api_ref": "https://huggingface.co/blog/gemma3n",
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://ai.google.dev/gemma/docs/gemma-3n",
  "source_weights_link": "https://huggingface.co/google/gemma-3n-E4B-it",
  "training_tokens": {
    "$numberLong": "11000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.517334+00:00",
  "avg_benchmark_score": 0.4202222222222222
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906660d"
  },
  "model_id": "gemma-3n-e4b-it-litert-preview",
  "announcement_date": "2025-05-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.451978+00:00",
  "description": "Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "gemma",
  "model_family_id": null,
  "model_name": "gemma-3n-e4b-it-litert-preview",
  "multimodal": true,
  "name": "Gemma 3n E4B Instructed LiteRT Preview",
  "organization": "google",
  "organization_id": "google",
  "param_count": 1910000000,
  "release_date": "2025-05-20",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": "https://aistudio.google.com/",
  "source_repo_link": "https://huggingface.co/google/gemma-3n-E4B-it-litert-preview",
  "source_scorecard_blog_link": "https://ai.google.dev/gemma/docs/gemma-3n",
  "source_weights_link": "https://huggingface.co/google/gemma-3n-E4B-it-litert-preview",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.451978+00:00",
  "avg_benchmark_score": 0.5029642857142858
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906660e"
  },
  "model_id": "medgemma-4b-it",
  "announcement_date": "2025-05-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.511963+00:00",
  "description": "MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "health_ai_developer_foundations_terms_of_use",
  "model_family_id": null,
  "model_name": "medgemma-4b-it",
  "multimodal": true,
  "name": "MedGemma 4B IT",
  "organization": "google",
  "organization_id": "google",
  "param_count": {
    "$numberLong": "4300000000"
  },
  "release_date": "2025-05-20",
  "source_api_ref": "https://developers.google.com/health-ai-developer-foundations/medgemma/get-started",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://developers.google.com/health-ai-developer-foundations/medgemma/model-card",
  "source_weights_link": "https://huggingface.co/google/medgemma-4b-it",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.511963+00:00",
  "avg_benchmark_score": 0.5851428571428572
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906660f"
  },
  "model_id": "granite-3.3-8b-base",
  "announcement_date": "2025-04-16",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.727013+00:00",
  "description": "Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-04-01",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "granite-3.3-8b-base",
  "multimodal": true,
  "name": "Granite 3.3 8B Base",
  "organization": "ibm",
  "organization_id": "ibm",
  "param_count": {
    "$numberLong": "8170000000"
  },
  "release_date": "2025-04-16",
  "source_api_ref": "https://www.ibm.com/granite/docs/",
  "source_paper": null,
  "source_playground": "https://www.ibm.com/granite/playground/",
  "source_repo_link": "https://github.com/ibm-granite/granite-3.3-language-models",
  "source_scorecard_blog_link": "https://huggingface.co/ibm-granite/granite-3.3-8b-base",
  "source_weights_link": "https://huggingface.co/ibm-granite/granite-3.3-8b-base",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.727013+00:00",
  "avg_benchmark_score": 0.6426999999999999
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066610"
  },
  "model_id": "granite-3.3-8b-instruct",
  "announcement_date": "2025-04-16",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.723958+00:00",
  "description": "Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-04-01",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "granite-3.3-8b-instruct",
  "multimodal": true,
  "name": "Granite 3.3 8B Instruct",
  "organization": "ibm",
  "organization_id": "ibm",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2025-04-16",
  "source_api_ref": "https://www.ibm.com/granite/docs/",
  "source_paper": null,
  "source_playground": "https://www.ibm.com/granite/playground/",
  "source_repo_link": "https://github.com/ibm-granite/granite-3.3-language-models",
  "source_scorecard_blog_link": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct",
  "source_weights_link": "https://huggingface.co/ibm-granite/granite-3.3-8b-instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.723958+00:00",
  "avg_benchmark_score": 0.69825
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066611"
  },
  "model_id": "granite-4.0-tiny-preview",
  "announcement_date": "2025-05-02",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.720766+00:00",
  "description": "A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "granite-4.0-tiny-preview",
  "multimodal": false,
  "name": "IBM Granite 4.0 Tiny Preview",
  "organization": "ibm",
  "organization_id": "ibm",
  "param_count": {
    "$numberLong": "7000000000"
  },
  "release_date": "2025-05-02",
  "source_api_ref": "https://www.ibm.com/granite/docs/",
  "source_paper": null,
  "source_playground": "https://www.ibm.com/granite/playground/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek",
  "source_weights_link": "https://huggingface.co/ibm-granite/granite-4.0-tiny-preview",
  "training_tokens": {
    "$numberLong": "2500000000000"
  },
  "updated_at": "2025-07-19T19:49:05.720766+00:00",
  "avg_benchmark_score": 0.5708833333333333
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066612"
  },
  "model_id": "llama-3.1-405b-instruct",
  "announcement_date": "2024-07-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.585389+00:00",
  "description": "Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "llama_3_1_community_license",
  "model_family_id": null,
  "model_name": "llama-3.1-405b-instruct",
  "multimodal": false,
  "name": "Llama 3.1 405B Instruct",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "405000000000"
  },
  "release_date": "2024-07-23",
  "source_api_ref": "https://github.com/meta-llama/llama-models",
  "source_paper": null,
  "source_playground": "https://llama.meta.com/llama-downloads",
  "source_repo_link": "https://github.com/meta-llama/llama-models",
  "source_scorecard_blog_link": "https://ai.meta.com/blog/meta-llama-3-1/",
  "source_weights_link": "https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct",
  "training_tokens": {
    "$numberLong": "15000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.585389+00:00",
  "avg_benchmark_score": 0.7918888888888889
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066613"
  },
  "model_id": "llama-3.1-70b-instruct",
  "announcement_date": "2024-07-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.575761+00:00",
  "description": "Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "llama_3_1_community_license",
  "model_family_id": null,
  "model_name": "llama-3.1-70b-instruct",
  "multimodal": false,
  "name": "Llama 3.1 70B Instruct",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "70000000000"
  },
  "release_date": "2024-07-23",
  "source_api_ref": "https://ai.meta.com/llama/",
  "source_paper": "https://ai.meta.com/research/publications/llama-3-open-foundation-and-fine-tuned-chat-models/",
  "source_playground": null,
  "source_repo_link": "https://github.com/meta-llama/llama-models",
  "source_scorecard_blog_link": "https://ai.meta.com/blog/meta-llama-3-1/",
  "source_weights_link": "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct",
  "training_tokens": {
    "$numberLong": "15000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.575761+00:00",
  "avg_benchmark_score": 0.7471111111111112
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066614"
  },
  "model_id": "llama-3.1-8b-instruct",
  "announcement_date": "2024-07-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.582878+00:00",
  "description": "Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-12-31",
  "license_id": "llama_3_1_community_license",
  "model_family_id": null,
  "model_name": "llama-3.1-8b-instruct",
  "multimodal": false,
  "name": "Llama 3.1 8B Instruct",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2024-07-23",
  "source_api_ref": "https://www.llama.com/",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/meta-llama/llama-models",
  "source_scorecard_blog_link": "https://ai.meta.com/blog/meta-llama-3-1/",
  "source_weights_link": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
  "training_tokens": {
    "$numberLong": "15000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.582878+00:00",
  "avg_benchmark_score": 0.6131666666666666
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066615"
  },
  "model_id": "llama-3.2-11b-instruct",
  "announcement_date": "2024-09-25",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.588479+00:00",
  "description": "Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-12-31",
  "license_id": "llama_3_2_community_license",
  "model_family_id": null,
  "model_name": "llama-3.2-11b-instruct",
  "multimodal": true,
  "name": "Llama 3.2 11B Instruct",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "10600000000"
  },
  "release_date": "2024-09-25",
  "source_api_ref": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/facebookresearch/llama",
  "source_scorecard_blog_link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
  "source_weights_link": "https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.588479+00:00",
  "avg_benchmark_score": 0.6362727272727272
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066616"
  },
  "model_id": "llama-3.2-3b-instruct",
  "announcement_date": "2024-09-25",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.591372+00:00",
  "description": "Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "llama_3_2_community_license",
  "model_family_id": null,
  "model_name": "llama-3.2-3b-instruct",
  "multimodal": false,
  "name": "Llama 3.2 3B Instruct",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "3210000000"
  },
  "release_date": "2024-09-25",
  "source_api_ref": "https://github.com/meta-llama/llama-models",
  "source_paper": null,
  "source_playground": "https://llama.meta.com/llama-downloads",
  "source_repo_link": "https://github.com/meta-llama/llama-models",
  "source_scorecard_blog_link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
  "source_weights_link": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
  "training_tokens": {
    "$numberLong": "9000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.591372+00:00",
  "avg_benchmark_score": 0.5560666666666666
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066617"
  },
  "model_id": "llama-3.2-90b-instruct",
  "announcement_date": "2024-09-25",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.579590+00:00",
  "description": "Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "llama3_2",
  "model_family_id": null,
  "model_name": "llama-3.2-90b-instruct",
  "multimodal": true,
  "name": "Llama 3.2 90B Instruct",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "90000000000"
  },
  "release_date": "2024-09-25",
  "source_api_ref": "https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct",
  "source_scorecard_blog_link": "https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/",
  "source_weights_link": "https://huggingface.co/meta-llama/Llama-3.2-90B-Vision-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.579590+00:00",
  "avg_benchmark_score": 0.7128461538461538
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066618"
  },
  "model_id": "llama-3.3-70b-instruct",
  "announcement_date": "2024-12-06",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.603412+00:00",
  "description": "Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "llama_3_3_community_license_agreement",
  "model_family_id": null,
  "model_name": "llama-3.3-70b-instruct",
  "multimodal": false,
  "name": "Llama 3.3 70B Instruct",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "70000000000"
  },
  "release_date": "2024-12-06",
  "source_api_ref": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",
  "source_paper": null,
  "source_playground": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",
  "source_repo_link": "https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",
  "training_tokens": {
    "$numberLong": "15000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.603412+00:00",
  "avg_benchmark_score": 0.7987777777777778
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066619"
  },
  "model_id": "llama-4-maverick",
  "announcement_date": "2025-04-05",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.595636+00:00",
  "description": "Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "llama_4_community_license_agreement",
  "model_family_id": null,
  "model_name": "llama-4-maverick",
  "multimodal": true,
  "name": "Llama 4 Maverick",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "400000000000"
  },
  "release_date": "2025-04-05",
  "source_api_ref": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
  "source_paper": null,
  "source_playground": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct",
  "source_repo_link": "https://github.com/meta-llama/llama-models/tree/main/models/llama4",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct",
  "training_tokens": {
    "$numberLong": "22000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.595636+00:00",
  "avg_benchmark_score": 0.7177692307692307
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906661a"
  },
  "model_id": "llama-4-scout",
  "announcement_date": "2025-04-05",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.599841+00:00",
  "description": "Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "llama_4_community_license_agreement",
  "model_family_id": null,
  "model_name": "llama-4-scout",
  "multimodal": true,
  "name": "Llama 4 Scout",
  "organization": "meta",
  "organization_id": "meta",
  "param_count": {
    "$numberLong": "109000000000"
  },
  "release_date": "2025-04-05",
  "source_api_ref": "https://ai.meta.com/blog/llama-4-multimodal-intelligence/",
  "source_paper": null,
  "source_playground": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",
  "source_repo_link": "https://github.com/meta-llama/llama-models/tree/main/models/llama4",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",
  "training_tokens": {
    "$numberLong": "40000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.599841+00:00",
  "avg_benchmark_score": 0.6728333333333333
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906661b"
  },
  "model_id": "phi-3.5-mini-instruct",
  "announcement_date": "2024-08-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.559796+00:00",
  "description": "Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-3.5-mini-instruct",
  "multimodal": false,
  "name": "Phi-3.5-mini-instruct",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "3800000000"
  },
  "release_date": "2024-08-23",
  "source_api_ref": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
  "source_paper": "https://arxiv.org/abs/2404.14219",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
  "training_tokens": {
    "$numberLong": "3400000000000"
  },
  "updated_at": "2025-07-19T19:49:05.559796+00:00",
  "avg_benchmark_score": 0.5871290322580646
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906661c"
  },
  "model_id": "phi-3.5-moe-instruct",
  "announcement_date": "2024-08-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.555819+00:00",
  "description": "Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-3.5-moe-instruct",
  "multimodal": false,
  "name": "Phi-3.5-MoE-instruct",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "60000000000"
  },
  "release_date": "2024-08-23",
  "source_api_ref": "https://huggingface.co/microsoft/Phi-3.5-MoE-instruct",
  "source_paper": "https://arxiv.org/abs/2404.14219",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-3.5-MoE-instruct",
  "training_tokens": {
    "$numberLong": "4900000000000"
  },
  "updated_at": "2025-07-19T19:49:05.555819+00:00",
  "avg_benchmark_score": 0.6555806451612903
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906661d"
  },
  "model_id": "phi-3.5-vision-instruct",
  "announcement_date": "2024-08-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.563203+00:00",
  "description": "Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-3.5-vision-instruct",
  "multimodal": true,
  "name": "Phi-3.5-vision-instruct",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "4200000000"
  },
  "release_date": "2024-08-23",
  "source_api_ref": "https://huggingface.co/microsoft/Phi-3.5-vision-instruct",
  "source_paper": "https://arxiv.org/abs/2404.14219",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://techcommunity.microsoft.com/blog/azure-ai-services-blog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/4225280",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-3.5-vision-instruct",
  "training_tokens": {
    "$numberLong": "500000000000"
  },
  "updated_at": "2025-07-19T19:49:05.563203+00:00",
  "avg_benchmark_score": 0.6826666666666666
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906661e"
  },
  "model_id": "phi-4",
  "announcement_date": "2024-12-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.549276+00:00",
  "description": "phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-4",
  "multimodal": false,
  "name": "Phi 4",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "14700000000"
  },
  "release_date": "2024-12-12",
  "source_api_ref": "https://huggingface.co/microsoft/phi-4",
  "source_paper": "https://arxiv.org/pdf/2412.08905",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090",
  "source_weights_link": "https://huggingface.co/microsoft/phi-4",
  "training_tokens": {
    "$numberLong": "9800000000000"
  },
  "updated_at": "2025-07-19T19:49:05.549276+00:00",
  "avg_benchmark_score": 0.6603076923076923
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906661f"
  },
  "model_id": "phi-4-mini",
  "announcement_date": "2025-02-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.552796+00:00",
  "description": "Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-4-mini",
  "multimodal": false,
  "name": "Phi 4 Mini",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "3840000000"
  },
  "release_date": "2025-02-01",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/pdf/2503.01743",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-4-mini-instruct",
  "training_tokens": {
    "$numberLong": "5000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.552796+00:00",
  "avg_benchmark_score": 0.6535294117647058
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066620"
  },
  "model_id": "phi-4-mini-reasoning",
  "announcement_date": "2025-04-30",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.545846+00:00",
  "description": "Phi-4-mini-reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-02-01",
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-4-mini-reasoning",
  "multimodal": false,
  "name": "Phi 4 Mini Reasoning",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "3800000000"
  },
  "release_date": "2025-04-30",
  "source_api_ref": "https://learn.microsoft.com/en-us/windows/ai/apis/phi-silica?tabs=csharp0,csharp1,csharp2,csharp3",
  "source_paper": "https://arxiv.org/pdf/2504.21233",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-4-mini-reasoning",
  "training_tokens": {
    "$numberLong": "150000000000"
  },
  "updated_at": "2025-07-19T19:49:05.545846+00:00",
  "avg_benchmark_score": 0.6803333333333333
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066621"
  },
  "model_id": "phi-4-multimodal-instruct",
  "announcement_date": "2025-02-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.571307+00:00",
  "description": "Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-4-multimodal-instruct",
  "multimodal": true,
  "name": "Phi-4-multimodal-instruct",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "5600000000"
  },
  "release_date": "2025-02-01",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/abs/2503.01743",
  "source_playground": "https://ai.azure.com/explore/models?selectedCollection=phi&tid=72f988bf-86f1-41af-91ab-2d7cd011db47",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://azure.microsoft.com/en-us/blog/empowering-innovation-the-next-generation-of-the-phi-family/",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-4-multimodal-instruct",
  "training_tokens": {
    "$numberLong": "5000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.571307+00:00",
  "avg_benchmark_score": 0.7202000000000001
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066622"
  },
  "model_id": "phi-4-reasoning",
  "announcement_date": "2025-04-30",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.879382+00:00",
  "description": "Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",
  "fine_tuned_from_model_id": "phi-4",
  "knowledge_cutoff": "2025-03-01",
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-4-reasoning",
  "multimodal": false,
  "name": "Phi 4 Reasoning",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "14000000000"
  },
  "release_date": "2025-04-30",
  "source_api_ref": "https://learn.microsoft.com/en-us/windows/ai/apis/phi-silica?tabs=csharp0,csharp1,csharp2,csharp3",
  "source_paper": "https://arxiv.org/abs/2504.21318",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-4-reasoning",
  "training_tokens": {
    "$numberLong": "16000000000"
  },
  "updated_at": "2025-07-19T19:49:05.879382+00:00",
  "avg_benchmark_score": 0.7514545454545455
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066623"
  },
  "model_id": "phi-4-reasoning-plus",
  "announcement_date": "2025-04-30",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.567534+00:00",
  "description": "Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-03-01",
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "phi-4-reasoning-plus",
  "multimodal": false,
  "name": "Phi 4 Reasoning Plus",
  "organization": "microsoft",
  "organization_id": "microsoft",
  "param_count": {
    "$numberLong": "14000000000"
  },
  "release_date": "2025-04-30",
  "source_api_ref": "https://learn.microsoft.com/en-us/windows/ai/apis/phi-silica?tabs=csharp0,csharp1,csharp2,csharp3",
  "source_paper": "https://arxiv.org/abs/2504.21318",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://azure.microsoft.com/en-us/blog/one-year-of-phi-small-language-models-making-big-leaps-in-ai/",
  "source_weights_link": "https://huggingface.co/microsoft/Phi-4-reasoning-plus",
  "training_tokens": {
    "$numberLong": "16000000000"
  },
  "updated_at": "2025-07-19T19:49:05.567534+00:00",
  "avg_benchmark_score": 0.7886363636363637
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066624"
  },
  "model_id": "codestral-22b",
  "announcement_date": "2024-05-29",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.805621+00:00",
  "description": "A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mnpl_0_1",
  "model_family_id": null,
  "model_name": "codestral-22b",
  "multimodal": false,
  "name": "Codestral-22B",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "22200000000"
  },
  "release_date": "2024-05-29",
  "source_api_ref": "https://docs.mistral.ai/api/",
  "source_paper": null,
  "source_playground": "https://chat.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/codestral/",
  "source_weights_link": "https://huggingface.co/mistralai/Codestral-22B-v0.1",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.805621+00:00",
  "avg_benchmark_score": 0.6588571428571429
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066625"
  },
  "model_id": "devstral-medium-2507",
  "announcement_date": "2025-07-10",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.783461+00:00",
  "description": "Devstral Medium builds upon the strengths of Devstral Small and takes performance to the next level with a score of 61.6% on SWE-Bench Verified. Devstral Medium is available through the Mistral public API, and offers exceptional performance at a competitive price point, making it an ideal choice for businesses and developers looking for a high-quality, cost-effective model.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "devstral-medium-2507",
  "multimodal": false,
  "name": "Devstral Medium",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": null,
  "release_date": "2025-07-10",
  "source_api_ref": "https://console.mistral.ai",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/devstral-2507",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.783461+00:00",
  "avg_benchmark_score": 0.616
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066626"
  },
  "model_id": "devstral-small-2507",
  "announcement_date": "2025-07-11",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.797947+00:00",
  "description": "Devstral Small 1.1 (also called devstral-small-2507) is based on the Mistral-Small-3.1 foundation model and contains approximately 24 billion parameters. It supports a 128k token context window, which allows it to handle multi-file code inputs and long prompts typical in software engineering workflows. The model is fine-tuned specifically for structured outputs, including XML and function-calling formats. This makes it compatible with agent frameworks such as OpenHands and suitable for tasks like program navigation, multi-step edits, and code search. It is licensed under Apache 2.0 and available for both research and commercial use.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "devstral-small-2507",
  "multimodal": false,
  "name": "Devstral Small 1.1",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "24000000000"
  },
  "release_date": "2025-07-11",
  "source_api_ref": "https://console.mistral.ai",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://huggingface.co/mistralai/Devstral-Small-2507",
  "source_weights_link": "https://huggingface.co/mistralai/Devstral-Small-2507/blob/main/model.safetensors.index.json",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.797947+00:00",
  "avg_benchmark_score": 0.536
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066627"
  },
  "model_id": "magistral-medium",
  "announcement_date": "2025-06-10",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.780565+00:00",
  "description": "Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-06-01",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "magistral-medium",
  "multimodal": true,
  "name": "Magistral Medium",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "24000000000"
  },
  "release_date": "2025-06-10",
  "source_api_ref": "https://docs.mistral.ai/api/",
  "source_paper": "https://arxiv.org/pdf/2506.10910",
  "source_playground": "https://chat.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/magistral",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.780565+00:00",
  "avg_benchmark_score": 0.5261666666666667
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066628"
  },
  "model_id": "magistral-small-2506",
  "announcement_date": "2025-06-10",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.777162+00:00",
  "description": "Building upon Mistral Small 3.1 (2503), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2025-06-01",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "magistral-small-2506",
  "multimodal": false,
  "name": "Magistral Small 2506",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "24000000000"
  },
  "release_date": "2025-06-10",
  "source_api_ref": "https://docs.mistral.ai/api/",
  "source_paper": "https://arxiv.org/pdf/2506.10910",
  "source_playground": "https://chat.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/magistral",
  "source_weights_link": "https://huggingface.co/mistralai/Magistral-Small-2506",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.777162+00:00",
  "avg_benchmark_score": 0.6323
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066629"
  },
  "model_id": "ministral-8b-instruct-2410",
  "announcement_date": "2024-10-16",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.786083+00:00",
  "description": "The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mistral_research_license",
  "model_family_id": null,
  "model_name": "ministral-8b-instruct-2410",
  "multimodal": false,
  "name": "Ministral 8B Instruct",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "8019808256"
  },
  "release_date": "2024-10-16",
  "source_api_ref": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/ministraux/",
  "source_weights_link": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.786083+00:00",
  "avg_benchmark_score": 0.6333636363636364
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906662a"
  },
  "model_id": "mistral-large-2-2407",
  "announcement_date": "2024-07-24",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.813974+00:00",
  "description": "A 123B parameter model with strong capabilities in code generation, mathematics, and reasoning. Features enhanced multilingual support across dozens of languages, 128k context window, and advanced function calling capabilities. Excels in instruction-following and maintains concise outputs.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mistral_research_license",
  "model_family_id": null,
  "model_name": "mistral-large-2-2407",
  "multimodal": false,
  "name": "Mistral Large 2",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "123000000000"
  },
  "release_date": "2024-07-24",
  "source_api_ref": "https://docs.mistral.ai/",
  "source_paper": null,
  "source_playground": "https://chat.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-large-2407/",
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.813974+00:00",
  "avg_benchmark_score": 0.8762000000000001
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906662b"
  },
  "model_id": "mistral-nemo-instruct-2407",
  "announcement_date": "2024-07-18",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.773595+00:00",
  "description": "A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "mistral-nemo-instruct-2407",
  "multimodal": false,
  "name": "Mistral NeMo Instruct",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "12000000000"
  },
  "release_date": "2024-07-18",
  "source_api_ref": "https://docs.mistral.ai/getting-started/models/models_overview/",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-nemo/",
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.773595+00:00",
  "avg_benchmark_score": 0.64325
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906662c"
  },
  "model_id": "mistral-small-2409",
  "announcement_date": "2024-09-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.809465+00:00",
  "description": "An enterprise-grade 22B parameter model optimized for tasks like translation, summarization, and sentiment analysis. Offers significant improvements in human alignment, reasoning capabilities, and code generation compared to previous versions.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mistral_research_license",
  "model_family_id": null,
  "model_name": "mistral-small-2409",
  "multimodal": false,
  "name": "Mistral Small",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "22000000000"
  },
  "release_date": "2024-09-17",
  "source_api_ref": "https://docs.mistral.ai/api/",
  "source_paper": null,
  "source_playground": "https://console.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/september-24-release/",
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-Instruct-2409",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.809465+00:00",
  "avg_benchmark_score": null
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906662d"
  },
  "model_id": "mistral-small-24b-base-2501",
  "announcement_date": "2025-01-30",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.791166+00:00",
  "description": "Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-10-01",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "mistral-small-24b-base-2501",
  "multimodal": true,
  "name": "Mistral Small 3 24B Base",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "23600000000"
  },
  "release_date": "2025-01-30",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": "https://console.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-small-3",
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.791166+00:00",
  "avg_benchmark_score": 0.6702555555555556
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906662e"
  },
  "model_id": "mistral-small-24b-instruct-2501",
  "announcement_date": "2025-01-30",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.788628+00:00",
  "description": "Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2â€“3Ã— its size while using significantly fewer compute resources.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-10-01",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "mistral-small-24b-instruct-2501",
  "multimodal": false,
  "name": "Mistral Small 3 24B Instruct",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "24000000000"
  },
  "release_date": "2025-01-30",
  "source_api_ref": "https://docs.mistral.ai/api/",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-small-3/",
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.788628+00:00",
  "avg_benchmark_score": 0.7165
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906662f"
  },
  "model_id": "mistral-small-3.1-24b-base-2503",
  "announcement_date": "2025-03-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.793911+00:00",
  "description": "Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "mistral-small-3.1-24b-base-2503",
  "multimodal": true,
  "name": "Mistral Small 3.1 24B Base",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "24000000000"
  },
  "release_date": "2025-03-17",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": "https://console.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-small-3-1",
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.793911+00:00",
  "avg_benchmark_score": 0.62862
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066630"
  },
  "model_id": "mistral-small-3.1-24b-instruct-2503",
  "announcement_date": "2025-03-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.770816+00:00",
  "description": "Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "mistral-small-3.1-24b-instruct-2503",
  "multimodal": true,
  "name": "Mistral Small 3.1 24B Instruct",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "24000000000"
  },
  "release_date": "2025-03-17",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": "https://console.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/mistral-small-3-1",
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.770816+00:00",
  "avg_benchmark_score": 0.6399555555555555
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066631"
  },
  "model_id": "mistral-small-3.2-24b-instruct-2506",
  "announcement_date": "2025-06-20",
  "available_in_zeroeval": true,
  "created_at": "2025-08-03T22:06:11.933573+00:00",
  "description": "Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.",
  "fine_tuned_from_model_id": "mistral-small-3.1-24b-base-2503",
  "knowledge_cutoff": "2023-10-01",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "mistral-small-3.2-24b-instruct-2506",
  "multimodal": true,
  "name": "Mistral Small 3.2 24B Instruct",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "23600000000"
  },
  "release_date": "2025-06-20",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": "https://console.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506",
  "training_tokens": null,
  "updated_at": "2025-08-03T22:06:11.933573+00:00",
  "avg_benchmark_score": 0.68164375
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066632"
  },
  "model_id": "pixtral-12b-2409",
  "announcement_date": "2024-09-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.802013+00:00",
  "description": "A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "pixtral-12b-2409",
  "multimodal": true,
  "name": "Pixtral-12B",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "12400000000"
  },
  "release_date": "2024-09-17",
  "source_api_ref": "https://docs.mistral.ai/platform/endpoints/",
  "source_paper": null,
  "source_playground": "https://chat.mistral.ai",
  "source_repo_link": "https://huggingface.co/mistralai/Pixtral-12B-2409",
  "source_scorecard_blog_link": "https://mistral.ai/news/pixtral-12b/",
  "source_weights_link": "https://huggingface.co/mistralai/Pixtral-12B-2409",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.802013+00:00",
  "avg_benchmark_score": 0.6685
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066633"
  },
  "model_id": "pixtral-large",
  "announcement_date": "2024-11-18",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.913427+00:00",
  "description": "A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",
  "fine_tuned_from_model_id": "mistral-large-2-2407",
  "knowledge_cutoff": null,
  "license_id": "mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use",
  "model_family_id": null,
  "model_name": "pixtral-large",
  "multimodal": true,
  "name": "Pixtral Large",
  "organization": "mistral",
  "organization_id": "mistral",
  "param_count": {
    "$numberLong": "124000000000"
  },
  "release_date": "2024-11-18",
  "source_api_ref": "https://mistral.ai/",
  "source_paper": null,
  "source_playground": "https://chat.mistral.ai/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://mistral.ai/news/pixtral-large/",
  "source_weights_link": "https://huggingface.co/mistralai/Pixtral-Large-Instruct-2411",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.913427+00:00",
  "avg_benchmark_score": 0.8049999999999999
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066634"
  },
  "model_id": "kimi-k1.5",
  "announcement_date": "2025-01-20",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.426406+00:00",
  "description": "Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "kimi-k1.5",
  "multimodal": true,
  "name": "Kimi-k1.5",
  "organization": "moonshotai",
  "organization_id": "moonshotai",
  "param_count": null,
  "release_date": "2025-01-20",
  "source_api_ref": "https://platform.moonshot.cn/docs/api-reference",
  "source_paper": "https://arxiv.org/abs/2501.12599",
  "source_playground": "https://kimi.ai/",
  "source_repo_link": "https://github.com/MoonshotAI/Kimi-k1.5",
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.426406+00:00",
  "avg_benchmark_score": 0.8171111111111111
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066635"
  },
  "model_id": "kimi-k2-0905",
  "announcement_date": "2025-09-05",
  "available_in_zeroeval": true,
  "created_at": "2025-09-14T00:00:00.000000+00:00",
  "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",
  "fine_tuned_from_model_id": "kimi-k2-instruct",
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "kimi-k2-0905",
  "multimodal": false,
  "name": "Kimi K2 0905",
  "organization": "moonshotai",
  "organization_id": "moonshotai",
  "param_count": {
    "$numberLong": "1000000000000"
  },
  "release_date": "2025-09-05",
  "source_api_ref": "https://docs.moonshot.cn/",
  "source_paper": null,
  "source_playground": "https://kimi.moonshot.cn/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://moonshot.cn/blog/kimi-k2-0905",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-09-14T00:00:00.000000+00:00",
  "avg_benchmark_score": 0.8246666666666668
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066636"
  },
  "model_id": "kimi-k2-base",
  "announcement_date": "2025-01-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.422399+00:00",
  "description": "Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "kimi-k2-base",
  "multimodal": false,
  "name": "Kimi K2 Base",
  "organization": "moonshotai",
  "organization_id": "moonshotai",
  "param_count": {
    "$numberLong": "1000000000000"
  },
  "release_date": "2025-01-01",
  "source_api_ref": "https://platform.moonshot.ai",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/MoonshotAI/Kimi-K2",
  "source_scorecard_blog_link": "https://moonshotai.github.io/Kimi-K2/",
  "source_weights_link": "https://huggingface.co/moonshotai/Kimi-K2-Base",
  "training_tokens": {
    "$numberLong": "15500000000000"
  },
  "updated_at": "2025-07-19T19:49:05.422399+00:00",
  "avg_benchmark_score": 0.6918461538461538
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066637"
  },
  "model_id": "kimi-k2-instruct",
  "announcement_date": "2025-01-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.875884+00:00",
  "description": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",
  "fine_tuned_from_model_id": "kimi-k2-base",
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "kimi-k2-instruct",
  "multimodal": false,
  "name": "Kimi K2 Instruct",
  "organization": "moonshotai",
  "organization_id": "moonshotai",
  "param_count": {
    "$numberLong": "1000000000000"
  },
  "release_date": "2025-01-01",
  "source_api_ref": "https://platform.moonshot.ai",
  "source_paper": null,
  "source_playground": "https://kimi.com",
  "source_repo_link": "https://github.com/MoonshotAI/Kimi-K2",
  "source_scorecard_blog_link": "https://moonshotai.github.io/Kimi-K2/",
  "source_weights_link": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
  "training_tokens": {
    "$numberLong": "15500000000000"
  },
  "updated_at": "2025-07-19T19:49:05.875884+00:00",
  "avg_benchmark_score": 0.666921052631579
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066638"
  },
  "model_id": "kimi-k2-instruct-0905",
  "announcement_date": "2025-09-05",
  "available_in_zeroeval": true,
  "created_at": "2025-09-05T00:00:00.000000+00:00",
  "description": "Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "kimi-k2-instruct-0905",
  "multimodal": false,
  "name": "Kimi K2-Instruct-0905",
  "organization": "moonshotai",
  "organization_id": "moonshotai",
  "param_count": {
    "$numberLong": "1000000000000"
  },
  "release_date": "2025-09-05",
  "source_api_ref": "https://platform.moonshot.ai",
  "source_paper": "https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf",
  "source_playground": "https://kimi.moonshot.cn/",
  "source_repo_link": "https://github.com/MoonshotAI/Kimi-K2",
  "source_scorecard_blog_link": "https://moonshotai.github.io/Kimi-K2/",
  "source_weights_link": "https://huggingface.co/MoonshotAI",
  "training_tokens": {
    "$numberLong": "15500000000000"
  },
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 0.6357272727272728
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066639"
  },
  "model_id": "llama-3.1-nemotron-70b-instruct",
  "announcement_date": "2024-10-01",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.908923+00:00",
  "description": "A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.",
  "fine_tuned_from_model_id": "llama-3.1-70b-instruct",
  "knowledge_cutoff": "2023-12-01",
  "license_id": "llama_3_1_community_license",
  "model_family_id": null,
  "model_name": "llama-3.1-nemotron-70b-instruct",
  "multimodal": false,
  "name": "Llama 3.1 Nemotron 70B Instruct",
  "organization": "nvidia",
  "organization_id": "nvidia",
  "param_count": {
    "$numberLong": "70000000000"
  },
  "release_date": "2024-10-01",
  "source_api_ref": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct",
  "source_paper": "https://arxiv.org/abs/2410.01257",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/",
  "source_weights_link": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.908923+00:00",
  "avg_benchmark_score": 0.6786090909090908
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906663a"
  },
  "model_id": "llama-3.1-nemotron-nano-8b-v1",
  "announcement_date": "2025-03-18",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.733231+00:00",
  "description": "Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-12-31",
  "license_id": "llama_3_1_community_license",
  "model_family_id": null,
  "model_name": "llama-3.1-nemotron-nano-8b-v1",
  "multimodal": false,
  "name": "Llama 3.1 Nemotron Nano 8B V1",
  "organization": "nvidia",
  "organization_id": "nvidia",
  "param_count": {
    "$numberLong": "8000000000"
  },
  "release_date": "2025-03-18",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/abs/2502.00203",
  "source_playground": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard",
  "source_weights_link": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.733231+00:00",
  "avg_benchmark_score": 0.7215714285714286
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906663b"
  },
  "model_id": "llama-3.1-nemotron-ultra-253b-v1",
  "announcement_date": "2025-04-07",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.735588+00:00",
  "description": "A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-12-01",
  "license_id": "llama_3_1_community_license",
  "model_family_id": null,
  "model_name": "llama-3.1-nemotron-ultra-253b-v1",
  "multimodal": false,
  "name": "Llama 3.1 Nemotron Ultra 253B v1",
  "organization": "nvidia",
  "organization_id": "nvidia",
  "param_count": {
    "$numberLong": "253000000000"
  },
  "release_date": "2025-04-07",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/abs/2502.00203",
  "source_playground": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard",
  "source_weights_link": "https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.735588+00:00",
  "avg_benchmark_score": 0.7922833333333333
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906663c"
  },
  "model_id": "llama-3.3-nemotron-super-49b-v1",
  "announcement_date": "2025-03-18",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.730826+00:00",
  "description": "Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-12-31",
  "license_id": "llama_3_1_community_license",
  "model_family_id": null,
  "model_name": "llama-3.3-nemotron-super-49b-v1",
  "multimodal": false,
  "name": "Llama-3.3 Nemotron Super 49B v1",
  "organization": "nvidia",
  "organization_id": "nvidia",
  "param_count": {
    "$numberLong": "49900000000"
  },
  "release_date": "2025-03-18",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/abs/2502.00203",
  "source_playground": "https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard",
  "source_weights_link": "https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.730826+00:00",
  "avg_benchmark_score": 0.8095285714285714
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906663d"
  },
  "model_id": "gpt-3.5-turbo-0125",
  "announcement_date": "2023-03-21",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.858492+00:00",
  "description": "The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2021-09-30",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-3.5-turbo-0125",
  "multimodal": false,
  "name": "GPT-3.5 Turbo",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2023-03-21",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-3-5-turbo",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.858492+00:00",
  "avg_benchmark_score": 0.42274999999999996
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906663e"
  },
  "model_id": "gpt-4-0613",
  "announcement_date": "2023-06-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.869531+00:00",
  "description": "GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2022-12-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4-0613",
  "multimodal": true,
  "name": "GPT-4",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2023-06-13",
  "source_api_ref": "https://platform.openai.com/docs/api-reference/chat",
  "source_paper": "https://arxiv.org/abs/2303.08774",
  "source_playground": "https://platform.openai.com/playground",
  "source_repo_link": "https://github.com/openai/gpt-4",
  "source_scorecard_blog_link": "https://openai.com/research/gpt-4",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.869531+00:00",
  "avg_benchmark_score": 0.7771666666666667
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906663f"
  },
  "model_id": "gpt-4-turbo-2024-04-09",
  "announcement_date": "2024-04-09",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.872559+00:00",
  "description": "The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-12-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4-turbo-2024-04-09",
  "multimodal": false,
  "name": "GPT-4 Turbo",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-04-09",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/new-models-and-developer-products-announced-at-devday/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.872559+00:00",
  "avg_benchmark_score": 0.7811666666666667
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066640"
  },
  "model_id": "gpt-4.1-2025-04-14",
  "announcement_date": "2025-04-14",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.841143+00:00",
  "description": "GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-06-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4.1-2025-04-14",
  "multimodal": true,
  "name": "GPT-4.1",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-04-14",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-4.1",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground?mode=chat&model=gpt-4.1",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/gpt-4-1/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.841143+00:00",
  "avg_benchmark_score": 0.5712758620689655
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066641"
  },
  "model_id": "gpt-4.1-mini-2025-04-14",
  "announcement_date": "2025-04-14",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.821382+00:00",
  "description": "GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-05-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4.1-mini-2025-04-14",
  "multimodal": true,
  "name": "GPT-4.1 mini",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-04-14",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-4.1-mini",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground?mode=chat&model=gpt-4.1-mini",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/gpt-4-1/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.821382+00:00",
  "avg_benchmark_score": 0.49777777777777776
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066642"
  },
  "model_id": "gpt-4.1-nano-2025-04-14",
  "announcement_date": "2025-04-14",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.827978+00:00",
  "description": "GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-05-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4.1-nano-2025-04-14",
  "multimodal": true,
  "name": "GPT-4.1 nano",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-04-14",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-4.1-nano",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground?mode=chat&model=gpt-4.1-nano",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/gpt-4-1/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.827978+00:00",
  "avg_benchmark_score": 0.351
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066643"
  },
  "model_id": "gpt-4.5",
  "announcement_date": "2025-02-27",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.852855+00:00",
  "description": "GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4.5",
  "multimodal": true,
  "name": "GPT-4.5",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-02-27",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-4-5#gpt-4-5",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground",
  "source_repo_link": "https://github.com/openai",
  "source_scorecard_blog_link": "https://openai.com/index/introducing-gpt-4-5/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.852855+00:00",
  "avg_benchmark_score": 0.6406153846153846
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066644"
  },
  "model_id": "gpt-4o-2024-05-13",
  "announcement_date": "2024-05-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.838358+00:00",
  "description": "GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4o-2024-05-13",
  "multimodal": true,
  "name": "GPT-4o",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-05-13",
  "source_api_ref": "https://platform.openai.com/docs/api-reference",
  "source_paper": null,
  "source_playground": "https://chat.openai.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/hello-gpt-4o/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.838358+00:00",
  "avg_benchmark_score": 0.77425
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066645"
  },
  "model_id": "gpt-4o-2024-08-06",
  "announcement_date": "2024-08-06",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.847621+00:00",
  "description": "GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4o-2024-08-06",
  "multimodal": true,
  "name": "GPT-4o",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-08-06",
  "source_api_ref": "https://platform.openai.com/docs/api-reference",
  "source_paper": null,
  "source_playground": "https://chat.openai.com/",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/hello-gpt-4o/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.847621+00:00",
  "avg_benchmark_score": 0.5300526315789473
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066646"
  },
  "model_id": "gpt-4o-mini-2024-07-18",
  "announcement_date": "2024-07-18",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.866393+00:00",
  "description": "GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-10-01",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-4o-mini-2024-07-18",
  "multimodal": true,
  "name": "GPT-4o mini",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-07-18",
  "source_api_ref": "https://platform.openai.com/docs/api-reference",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.866393+00:00",
  "avg_benchmark_score": 0.6345555555555555
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066647"
  },
  "model_id": "gpt-5-2025-08-07",
  "announcement_date": "2025-08-07",
  "available_in_zeroeval": true,
  "created_at": "2025-07-24T12:00:00.000000+00:00",
  "description": "GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-09-30",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-5-2025-08-07",
  "multimodal": true,
  "name": "GPT-5",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-08-07",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-5",
  "source_paper": "https://cdn.openai.com/pdf/8124a3ce-ab78-4f06-96eb-49ea29ffb52f/gpt5-system-card-aug7.pdf",
  "source_playground": "https://platform.openai.com/playground?mode=chat&model=gpt-5",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/gpt-5/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-24T12:00:00.000000+00:00",
  "avg_benchmark_score": 0.7008285714285715
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066648"
  },
  "model_id": "gpt-5-mini-2025-08-07",
  "announcement_date": "2025-08-07",
  "available_in_zeroeval": true,
  "created_at": "2025-07-24T12:00:00.000000+00:00",
  "description": "A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for well-defined tasks and precise prompts with high reasoning capabilities at reduced cost.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-05-30",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-5-mini-2025-08-07",
  "multimodal": true,
  "name": "GPT-5 mini",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-08-07",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-5-mini",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground?mode=chat&model=gpt-5-mini",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/gpt-5/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-24T12:00:00.000000+00:00",
  "avg_benchmark_score": 0.6
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066649"
  },
  "model_id": "gpt-5-nano-2025-08-07",
  "announcement_date": "2025-08-07",
  "available_in_zeroeval": true,
  "created_at": "2025-07-24T12:00:00.000000+00:00",
  "description": "GPT-5 nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks with average reasoning capabilities and very fast speed.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-05-30",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "gpt-5-nano-2025-08-07",
  "multimodal": true,
  "name": "GPT-5 nano",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-08-07",
  "source_api_ref": "https://platform.openai.com/docs/models/gpt-5-nano",
  "source_paper": null,
  "source_playground": "https://platform.openai.com/playground?mode=chat&model=gpt-5-nano",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/gpt-5/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-24T12:00:00.000000+00:00",
  "avg_benchmark_score": 0.5006
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906664a"
  },
  "model_id": "gpt-oss-120b",
  "announcement_date": "2025-08-05",
  "available_in_zeroeval": true,
  "created_at": "2025-08-05T19:49:05.852855+00:00",
  "description": "GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "gpt-oss-120b",
  "multimodal": false,
  "name": "GPT OSS 120B",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": {
    "$numberLong": "116800000000"
  },
  "release_date": "2025-08-05",
  "source_api_ref": null,
  "source_paper": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
  "source_playground": "https://gpt-oss.com/",
  "source_repo_link": "https://github.com/openai/gpt-oss",
  "source_scorecard_blog_link": "https://openai.com/index/gpt-oss-model-card/",
  "source_weights_link": "https://huggingface.co/openai/gpt-oss-120b",
  "training_tokens": null,
  "updated_at": "2025-08-05T19:49:05.852855+00:00",
  "avg_benchmark_score": null
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906664b"
  },
  "model_id": "gpt-oss-20b",
  "announcement_date": "2025-08-05",
  "available_in_zeroeval": true,
  "created_at": "2025-08-05T19:49:05.852855+00:00",
  "description": "The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3â€‘mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPTâ€‘4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "gpt-oss-20b",
  "multimodal": false,
  "name": "GPT OSS 20B",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": {
    "$numberLong": "20900000000"
  },
  "release_date": "2025-08-05",
  "source_api_ref": null,
  "source_paper": "https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf",
  "source_playground": "https://gpt-oss.com/",
  "source_repo_link": "https://github.com/openai/gpt-oss",
  "source_scorecard_blog_link": "https://openai.com/index/gpt-oss-model-card/",
  "source_weights_link": "https://huggingface.co/openai/gpt-oss-20b",
  "training_tokens": null,
  "updated_at": "2025-08-05T19:49:05.852855+00:00",
  "avg_benchmark_score": 0.449
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906664c"
  },
  "model_id": "o1-2024-12-17",
  "announcement_date": "2024-12-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.855348+00:00",
  "description": "A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o1-2024-12-17",
  "multimodal": false,
  "name": "o1",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-12-17",
  "source_api_ref": "https://platform.openai.com/docs/models",
  "source_paper": "https://cdn.openai.com/o1-system-card-20240917.pdf",
  "source_playground": null,
  "source_repo_link": "https://openai.com/index/o1-and-new-tools-for-developers/",
  "source_scorecard_blog_link": "https://openai.com/index/learning-to-reason-with-llms",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.855348+00:00",
  "avg_benchmark_score": 0.7158421052631578
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906664d"
  },
  "model_id": "o1-mini",
  "announcement_date": "2024-09-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.850010+00:00",
  "description": "o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o1-mini",
  "multimodal": false,
  "name": "o1-mini",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-09-12",
  "source_api_ref": "https://openai.com/api/o1-mini",
  "source_paper": "https://cdn.openai.com/o1-system-card-20240917.pdf",
  "source_playground": "https://platform.openai.com/playground",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.850010+00:00",
  "avg_benchmark_score": 0.7188333333333333
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906664e"
  },
  "model_id": "o1-preview",
  "announcement_date": "2024-09-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.862671+00:00",
  "description": "A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o1-preview",
  "multimodal": false,
  "name": "o1-preview",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-09-12",
  "source_api_ref": "https://platform.openai.com/docs/models",
  "source_paper": "https://cdn.openai.com/o1-system-card-20240917.pdf",
  "source_playground": null,
  "source_repo_link": "https://github.com/openai",
  "source_scorecard_blog_link": "https://openai.com/index/learning-to-reason-with-llms",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.862671+00:00",
  "avg_benchmark_score": 0.648
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906664f"
  },
  "model_id": "o1-pro",
  "announcement_date": "2024-12-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.844613+00:00",
  "description": "o1-pro is OpenAI's advanced language model optimized for complex reasoning and specialized professional tasks, offering enhanced capabilities while maintaining high efficiency.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-09-30",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o1-pro",
  "multimodal": true,
  "name": "o1-pro",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2024-12-17",
  "source_api_ref": "https://openai.com/api",
  "source_paper": "https://cdn.openai.com/o1-system-card-20240917.pdf",
  "source_playground": "https://platform.openai.com/playground",
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://openai.com/index/introducing-chatgpt-pro/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.844613+00:00",
  "avg_benchmark_score": 0.825
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066650"
  },
  "model_id": "o3-2025-04-16",
  "announcement_date": "2025-04-16",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.818000+00:00",
  "description": "OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-05-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o3-2025-04-16",
  "multimodal": true,
  "name": "o3",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-04-16",
  "source_api_ref": "https://platform.openai.com/docs/models/o3",
  "source_paper": "https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf",
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.818000+00:00",
  "avg_benchmark_score": 0.63376
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066651"
  },
  "model_id": "o3-mini",
  "announcement_date": "2025-01-30",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.835007+00:00",
  "description": "A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-09-30",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o3-mini",
  "multimodal": false,
  "name": "o3-mini",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-01-30",
  "source_api_ref": "https://platform.openai.com/docs/models",
  "source_paper": "https://cdn.openai.com/o3-mini-system-card.pdf",
  "source_playground": null,
  "source_repo_link": "https://github.com/openai",
  "source_scorecard_blog_link": "https://openai.com/index/openai-o3-mini/",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.835007+00:00",
  "avg_benchmark_score": 0.5693846153846154
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066652"
  },
  "model_id": "o3-pro-2025-06-10",
  "announcement_date": "2025-06-10",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.832229+00:00",
  "description": "Version of o3 with more compute for better responses. The o3-pro model uses more compute to think harder and provide consistently better answers. Designed to tackle tough problems with advanced reasoning capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-05-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o3-pro-2025-06-10",
  "multimodal": true,
  "name": "o3-pro",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-06-10",
  "source_api_ref": "https://platform.openai.com/docs/models/o3-pro",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.832229+00:00",
  "avg_benchmark_score": null
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066653"
  },
  "model_id": "o4-mini",
  "announcement_date": "2025-04-16",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.824485+00:00",
  "description": "o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-05-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "o4-mini",
  "multimodal": true,
  "name": "o4-mini",
  "organization": "openai",
  "organization_id": "openai",
  "param_count": null,
  "release_date": "2025-04-16",
  "source_api_ref": "https://platform.openai.com/docs/models/o4-mini",
  "source_paper": "https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf",
  "source_playground": null,
  "source_repo_link": "https://github.com/openai",
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.824485+00:00",
  "avg_benchmark_score": 0.6648571428571428
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066654"
  },
  "model_id": "qvq-72b-preview",
  "announcement_date": "2024-12-25",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.895366+00:00",
  "description": "An experimental research model focusing on advanced visual reasoning and step-by-step cognitive capabilities. Achieves strong performance on multi-modal science and mathematics tasks, though exhibits some limitations such as potential language mixing and recursive reasoning loops.",
  "fine_tuned_from_model_id": "qwen2-vl-72b",
  "knowledge_cutoff": null,
  "license_id": "qwen",
  "model_family_id": null,
  "model_name": "qvq-72b-preview",
  "multimodal": true,
  "name": "QvQ-72B-Preview",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "73400000000"
  },
  "release_date": "2024-12-25",
  "source_api_ref": "https://huggingface.co/Qwen/QVQ-72B-Preview",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qvq-72b-preview/",
  "source_weights_link": "https://huggingface.co/Qwen/QVQ-72B-Preview",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.895366+00:00",
  "avg_benchmark_score": 0.495
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066655"
  },
  "model_id": "qwen-2.5-14b-instruct",
  "announcement_date": "2024-09-19",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.615575+00:00",
  "description": "Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen-2.5-14b-instruct",
  "multimodal": false,
  "name": "Qwen2.5 14B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "14700000000"
  },
  "release_date": "2024-09-19",
  "source_api_ref": "https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api",
  "source_paper": "https://arxiv.org/abs/2407.10671",
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-14B-Instruct",
  "training_tokens": {
    "$numberLong": "18000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.615575+00:00",
  "avg_benchmark_score": 0.6998125
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066656"
  },
  "model_id": "qwen-2.5-32b-instruct",
  "announcement_date": "2024-09-19",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.606261+00:00",
  "description": "Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen-2.5-32b-instruct",
  "multimodal": false,
  "name": "Qwen2.5 32B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "32500000000"
  },
  "release_date": "2024-09-19",
  "source_api_ref": "https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct",
  "training_tokens": {
    "$numberLong": "18000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.606261+00:00",
  "avg_benchmark_score": 0.7427777777777778
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066657"
  },
  "model_id": "qwen-2.5-72b-instruct",
  "announcement_date": "2024-09-19",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.627855+00:00",
  "description": "Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "qwen",
  "model_family_id": null,
  "model_name": "qwen-2.5-72b-instruct",
  "multimodal": false,
  "name": "Qwen2.5 72B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "72700000000"
  },
  "release_date": "2024-09-19",
  "source_api_ref": "https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct",
  "training_tokens": {
    "$numberLong": "18000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.627855+00:00",
  "avg_benchmark_score": 0.7742142857142857
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066658"
  },
  "model_id": "qwen-2.5-7b-instruct",
  "announcement_date": "2024-09-19",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.642960+00:00",
  "description": "Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen-2.5-7b-instruct",
  "multimodal": false,
  "name": "Qwen2.5 7B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "7610000000"
  },
  "release_date": "2024-09-19",
  "source_api_ref": "https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api",
  "source_paper": "https://arxiv.org/abs/2407.10671",
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-llm/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
  "training_tokens": {
    "$numberLong": "18000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.642960+00:00",
  "avg_benchmark_score": 0.6558571428571429
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066659"
  },
  "model_id": "qwen-2.5-coder-32b-instruct",
  "announcement_date": "2024-09-19",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.882455+00:00",
  "description": "Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",
  "fine_tuned_from_model_id": "qwen-2.5-32b-instruct",
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen-2.5-coder-32b-instruct",
  "multimodal": false,
  "name": "Qwen2.5-Coder 32B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "32000000000"
  },
  "release_date": "2024-09-19",
  "source_api_ref": "https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api",
  "source_paper": "https://arxiv.org/abs/2409.12186",
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5-Coder",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-coder/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-Coder-32B",
  "training_tokens": {
    "$numberLong": "5500000000000"
  },
  "updated_at": "2025-07-19T19:49:05.882455+00:00",
  "avg_benchmark_score": 0.6492
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906665a"
  },
  "model_id": "qwen-2.5-coder-7b-instruct",
  "announcement_date": "2024-09-19",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.890300+00:00",
  "description": "Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",
  "fine_tuned_from_model_id": "qwen-2.5-7b-instruct",
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen-2.5-coder-7b-instruct",
  "multimodal": false,
  "name": "Qwen2.5-Coder 7B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "7000000000"
  },
  "release_date": "2024-09-19",
  "source_api_ref": "https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api",
  "source_paper": "https://arxiv.org/abs/2409.12186",
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-coder/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-7B-Coder",
  "training_tokens": {
    "$numberLong": "5500000000000"
  },
  "updated_at": "2025-07-19T19:49:05.890300+00:00",
  "avg_benchmark_score": 0.5795789473684211
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906665b"
  },
  "model_id": "qwen2-72b-instruct",
  "announcement_date": "2024-07-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.650844+00:00",
  "description": "Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "tongyi_qianwen",
  "model_family_id": null,
  "model_name": "qwen2-72b-instruct",
  "multimodal": false,
  "name": "Qwen2 72B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "72000000000"
  },
  "release_date": "2024-07-23",
  "source_api_ref": "https://huggingface.co/Qwen/Qwen2-72B",
  "source_paper": "https://arxiv.org/abs/2309.00071",
  "source_playground": "https://huggingface.co/Qwen/Qwen2-72B",
  "source_repo_link": "https://huggingface.co/Qwen/Qwen2-72B",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2-72B",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.650844+00:00",
  "avg_benchmark_score": 0.7361176470588235
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906665c"
  },
  "model_id": "qwen2-7b-instruct",
  "announcement_date": "2024-07-23",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.612662+00:00",
  "description": "Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen2-7b-instruct",
  "multimodal": false,
  "name": "Qwen2 7B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "7620000000"
  },
  "release_date": "2024-07-23",
  "source_api_ref": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
  "source_paper": "https://arxiv.org/abs/2309.00071",
  "source_playground": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
  "source_repo_link": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.612662+00:00",
  "avg_benchmark_score": 0.5954285714285714
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906665d"
  },
  "model_id": "qwen2-vl-72b",
  "announcement_date": "2024-08-29",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.619575+00:00",
  "description": "An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2023-06-30",
  "license_id": "tongyi_qianwen",
  "model_family_id": null,
  "model_name": "qwen2-vl-72b",
  "multimodal": true,
  "name": "Qwen2-VL-72B-Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "73400000000"
  },
  "release_date": "2024-08-29",
  "source_api_ref": "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct",
  "source_paper": "https://arxiv.org/abs/2409.12191",
  "source_playground": null,
  "source_repo_link": "https://github.com/QwenLM/Qwen2-VL",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2-vl/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.619575+00:00",
  "avg_benchmark_score": 0.7575533333333334
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906665e"
  },
  "model_id": "qwen2.5-omni-7b",
  "announcement_date": "2025-03-27",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.639433+00:00",
  "description": "Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen2.5-omni-7b",
  "multimodal": true,
  "name": "Qwen2.5-Omni-7B",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "7000000000"
  },
  "release_date": "2025-03-27",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/pdf/2503.20215",
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5-Omni",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-omni/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-Omni-7B",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.639433+00:00",
  "avg_benchmark_score": 0.5922844444444444
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906665f"
  },
  "model_id": "qwen2.5-vl-32b",
  "announcement_date": "2025-02-28",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.653921+00:00",
  "description": "Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen2.5-vl-32b",
  "multimodal": true,
  "name": "Qwen2.5 VL 32B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "33500000000"
  },
  "release_date": "2025-02-28",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/pdf/2502.13923",
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5-VL",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-vl/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.653921+00:00",
  "avg_benchmark_score": 0.6356964285714285
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066660"
  },
  "model_id": "qwen2.5-vl-72b",
  "announcement_date": "2025-01-26",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.647509+00:00",
  "description": "Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "tongyi_qianwen",
  "model_family_id": null,
  "model_name": "qwen2.5-vl-72b",
  "multimodal": true,
  "name": "Qwen2.5 VL 72B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "72000000000"
  },
  "release_date": "2025-01-26",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/pdf/2502.13923",
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5-VL",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-vl/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.647509+00:00",
  "avg_benchmark_score": 0.66902
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066661"
  },
  "model_id": "qwen2.5-vl-7b",
  "announcement_date": "2025-01-26",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.635630+00:00",
  "description": "Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen2.5-vl-7b",
  "multimodal": true,
  "name": "Qwen2.5 VL 7B Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "8290000000"
  },
  "release_date": "2025-01-26",
  "source_api_ref": null,
  "source_paper": "https://arxiv.org/pdf/2502.13923",
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen2.5-VL",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen2.5-vl/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.635630+00:00",
  "avg_benchmark_score": 0.6452375
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066662"
  },
  "model_id": "qwen3-235b-a22b",
  "announcement_date": "2025-04-29",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.624683+00:00",
  "description": "Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen3-235b-a22b",
  "multimodal": false,
  "name": "Qwen3 235B A22B",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "235000000000"
  },
  "release_date": "2025-04-29",
  "source_api_ref": "https://qwenlm.github.io/blog/qwen3/",
  "source_paper": null,
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_scorecard_blog_link": null,
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B",
  "training_tokens": {
    "$numberLong": "36000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.624683+00:00",
  "avg_benchmark_score": 0.7620652173913044
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066663"
  },
  "model_id": "qwen3-235b-a22b-instruct-2507",
  "announcement_date": "2025-07-22",
  "available_in_zeroeval": true,
  "created_at": "2025-08-03T22:06:11.701778+00:00",
  "description": "Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen3-235b-a22b-instruct-2507",
  "multimodal": false,
  "name": "Qwen3-235B-A22B-Instruct-2507",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "235000000000"
  },
  "release_date": "2025-07-22",
  "source_api_ref": "https://qwenlm.github.io/blog/qwen3/",
  "source_paper": "https://arxiv.org/abs/2505.09388",
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507",
  "training_tokens": null,
  "updated_at": "2025-08-03T22:06:11.701778+00:00",
  "avg_benchmark_score": 0.72124
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066664"
  },
  "model_id": "qwen3-235b-a22b-thinking-2507",
  "announcement_date": "2025-07-25",
  "available_in_zeroeval": true,
  "created_at": "2025-07-25T00:00:00.000000+00:00",
  "description": "Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",
  "fine_tuned_from_model_id": "qwen3-235b-a22b",
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen3-235b-a22b-thinking-2507",
  "multimodal": false,
  "name": "Qwen3-235B-A22B-Thinking-2507",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "235000000000"
  },
  "release_date": "2025-07-25",
  "source_api_ref": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
  "source_paper": null,
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3-thinking/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507",
  "training_tokens": null,
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 86.04376
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066665"
  },
  "model_id": "qwen3-30b-a3b",
  "announcement_date": "2025-04-29",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.631206+00:00",
  "description": "Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen3-30b-a3b",
  "multimodal": false,
  "name": "Qwen3 30B A3B",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "30500000000"
  },
  "release_date": "2025-04-29",
  "source_api_ref": "https://qwenlm.github.io/blog/qwen3/",
  "source_paper": null,
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-30B-A3B",
  "training_tokens": {
    "$numberLong": "36000000000000"
  },
  "updated_at": "2025-07-19T19:49:05.631206+00:00",
  "avg_benchmark_score": 0.7328749999999999
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066666"
  },
  "model_id": "qwen3-32b",
  "announcement_date": "2025-04-29",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.621845+00:00",
  "description": "Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen3-32b",
  "multimodal": false,
  "name": "Qwen3 32B",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "32800000000"
  },
  "release_date": "2025-04-29",
  "source_api_ref": null,
  "source_paper": null,
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-32B",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.621845+00:00",
  "avg_benchmark_score": 0.7201111111111111
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066667"
  },
  "model_id": "qwen3-next-80b-a3b-instruct",
  "announcement_date": "2025-01-10",
  "available_in_zeroeval": true,
  "created_at": "2025-01-10T00:00:00.000000+00:00",
  "description": "Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen3-next-80b-a3b-instruct",
  "multimodal": false,
  "name": "Qwen3-Next-80B-A3B-Instruct",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "80000000000"
  },
  "release_date": "2025-01-10",
  "source_api_ref": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct",
  "source_paper": null,
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3-next/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct",
  "training_tokens": {
    "$numberLong": "15000000000000"
  },
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 0.6700833333333334
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066668"
  },
  "model_id": "qwen3-next-80b-a3b-thinking",
  "announcement_date": "2025-01-10",
  "available_in_zeroeval": true,
  "created_at": "2025-01-10T00:00:00.000000+00:00",
  "description": "Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks â€” outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwen3-next-80b-a3b-thinking",
  "multimodal": false,
  "name": "Qwen3-Next-80B-A3B-Thinking",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "80000000000"
  },
  "release_date": "2025-01-10",
  "source_api_ref": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking",
  "source_paper": null,
  "source_playground": "https://chat.qwen.ai/",
  "source_repo_link": "https://github.com/QwenLM/Qwen3",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwen3-next/",
  "source_weights_link": "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking",
  "training_tokens": {
    "$numberLong": "15000000000000"
  },
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 90.71313043478261
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066669"
  },
  "model_id": "qwq-32b",
  "announcement_date": "2025-03-05",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.609393+00:00",
  "description": "A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-11-28",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwq-32b",
  "multimodal": false,
  "name": "QwQ-32B",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "32500000000"
  },
  "release_date": "2025-03-05",
  "source_api_ref": "https://huggingface.co/Qwen/QwQ-32B",
  "source_paper": "https://arxiv.org/abs/2412.15115",
  "source_playground": "https://huggingface.co/playground?modelId=Qwen/QwQ-32B",
  "source_repo_link": "https://github.com/QwenLM/QwQ",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwq-32b/",
  "source_weights_link": "https://huggingface.co/Qwen/QwQ-32B",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.609393+00:00",
  "avg_benchmark_score": 0.7458571428571429
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906666a"
  },
  "model_id": "qwq-32b-preview",
  "announcement_date": "2024-11-28",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.887027+00:00",
  "description": "An experimental research model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive reasoning patterns.",
  "fine_tuned_from_model_id": "qwen-2.5-32b-instruct",
  "knowledge_cutoff": "2024-11-28",
  "license_id": "apache_2_0",
  "model_family_id": null,
  "model_name": "qwq-32b-preview",
  "multimodal": false,
  "name": "QwQ-32B-Preview",
  "organization": "qwen",
  "organization_id": "qwen",
  "param_count": {
    "$numberLong": "32500000000"
  },
  "release_date": "2024-11-28",
  "source_api_ref": "https://huggingface.co/Qwen/QwQ-32B-Preview",
  "source_paper": "https://arxiv.org/abs/2407.10671",
  "source_playground": "https://huggingface.co/spaces/Qwen/QwQ-32B-Preview",
  "source_repo_link": "https://github.com/QwenLM/Qwen2",
  "source_scorecard_blog_link": "https://qwenlm.github.io/blog/qwq-32b-preview/",
  "source_weights_link": "https://huggingface.co/Qwen/QwQ-32B-Preview",
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.887027+00:00",
  "avg_benchmark_score": 0.6395
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906666b"
  },
  "model_id": "grok-1.5",
  "announcement_date": "2024-03-28",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.705047+00:00",
  "description": "An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-1.5",
  "multimodal": false,
  "name": "Grok-1.5",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2024-03-28",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": "https://github.com/xai-org/grok-1",
  "source_scorecard_blog_link": "https://x.ai/blog/grok-1.5",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.705047+00:00",
  "avg_benchmark_score": 0.6387777777777778
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906666c"
  },
  "model_id": "grok-1.5v",
  "announcement_date": "2024-04-12",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.717803+00:00",
  "description": "A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-1.5v",
  "multimodal": true,
  "name": "Grok-1.5V",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2024-04-12",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://x.ai/blog/grok-1.5v",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.717803+00:00",
  "avg_benchmark_score": 0.7188571428571429
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906666d"
  },
  "model_id": "grok-2",
  "announcement_date": "2024-08-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.715016+00:00",
  "description": "Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-2",
  "multimodal": true,
  "name": "Grok-2",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2024-08-13",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://x.ai/blog/grok-2",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.715016+00:00",
  "avg_benchmark_score": 0.76525
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906666e"
  },
  "model_id": "grok-2-mini",
  "announcement_date": "2024-08-13",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.702680+00:00",
  "description": "Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-2-mini",
  "multimodal": true,
  "name": "Grok-2 mini",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2024-08-13",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": "https://x.ai/blog/grok-2",
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.702680+00:00",
  "avg_benchmark_score": 0.7405
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e906666f"
  },
  "model_id": "grok-3",
  "announcement_date": "2025-02-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.711845+00:00",
  "description": "Grok 3, launched by xAI on February 17, 2025, is an advanced AI model with significantly enhanced capabilities compared to Grok 2, boasting an order of magnitude increase in performance. Trained on a vast dataset that includes legal documents among others, and utilizing a massive compute infrastructure with around 200,000 GPUs in a Memphis data center, Grok 3's training used ten times more compute than its predecessor. It features specialized models like Grok 3 Reasoning and Grok 3 Mini Reasoning for complex problem-solving, and it excels in benchmarks like AIME for mathematics and GPQA for PhD-level science.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-11-17",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-3",
  "multimodal": true,
  "name": "Grok-3",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2025-02-17",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.711845+00:00",
  "avg_benchmark_score": 0.8572000000000001
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066670"
  },
  "model_id": "grok-3-mini",
  "announcement_date": "2025-02-17",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.697297+00:00",
  "description": "Grok 3 Mini is a streamlined version of xAI's Grok 3 AI model, designed for quicker response times while maintaining utility. It's tailored for users who require speed over the comprehensive capabilities of the full Grok 3 model, making it suitable for tasks where rapid information retrieval is key. Grok 3 Mini still leverages the advanced training and data that Grok 3 was built on but offers a lighter, more efficient version for everyday use.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-11-17",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-3-mini",
  "multimodal": true,
  "name": "Grok-3 Mini",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2025-02-17",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.697297+00:00",
  "avg_benchmark_score": 0.8775
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066671"
  },
  "model_id": "grok-4",
  "announcement_date": "2025-07-09",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.707962+00:00",
  "description": "Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-12-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-4",
  "multimodal": true,
  "name": "Grok-4",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2025-07-09",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.707962+00:00",
  "avg_benchmark_score": 0.6308571428571429
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066672"
  },
  "model_id": "grok-4-heavy",
  "announcement_date": "2025-07-09",
  "available_in_zeroeval": true,
  "created_at": "2025-07-19T19:49:05.700416+00:00",
  "description": "Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": "2024-12-31",
  "license_id": "proprietary",
  "model_family_id": null,
  "model_name": "grok-4-heavy",
  "multimodal": true,
  "name": "Grok-4 Heavy",
  "organization": "xai",
  "organization_id": "xai",
  "param_count": null,
  "release_date": "2025-07-09",
  "source_api_ref": "https://x.ai/api",
  "source_paper": null,
  "source_playground": null,
  "source_repo_link": null,
  "source_scorecard_blog_link": null,
  "source_weights_link": null,
  "training_tokens": null,
  "updated_at": "2025-07-19T19:49:05.700416+00:00",
  "avg_benchmark_score": 0.7951666666666667
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066673"
  },
  "model_id": "glm-4.5",
  "announcement_date": "2025-07-28",
  "available_in_zeroeval": true,
  "created_at": "2025-09-15T00:00:00.000000+00:00",
  "description": "GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "glm-4.5",
  "multimodal": false,
  "name": "GLM-4.5",
  "organization": "zai-org",
  "organization_id": "zai-org",
  "param_count": {
    "$numberLong": "355000000000"
  },
  "release_date": "2025-07-28",
  "source_api_ref": "https://docs.z.ai/guides/llm/glm-4.5",
  "source_paper": "https://arxiv.org/pdf/2508.06471",
  "source_playground": "https://chat.z.ai",
  "source_repo_link": "https://github.com/zai-org/GLM-4.5",
  "source_scorecard_blog_link": "https://z.ai/blog/glm-4.5",
  "source_weights_link": "https://huggingface.co/zai-org/GLM-4.5",
  "training_tokens": {
    "$numberLong": "23000000000000"
  },
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 0.6397142857142857
},
{
  "_id": {
    "$oid": "69255674b7cff6f0e9066674"
  },
  "model_id": "glm-4.5-air",
  "announcement_date": "2025-07-28",
  "available_in_zeroeval": true,
  "created_at": "2025-09-15T00:00:00.000000+00:00",
  "description": "GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",
  "fine_tuned_from_model_id": null,
  "knowledge_cutoff": null,
  "license_id": "mit",
  "model_family_id": null,
  "model_name": "glm-4.5-air",
  "multimodal": false,
  "name": "GLM-4.5-Air",
  "organization": "zai-org",
  "organization_id": "zai-org",
  "param_count": {
    "$numberLong": "106000000000"
  },
  "release_date": "2025-07-28",
  "source_api_ref": "https://docs.z.ai/guides/llm/glm-4.5",
  "source_paper": "https://arxiv.org/pdf/2508.06471",
  "source_playground": "https://chat.z.ai",
  "source_repo_link": "https://github.com/zai-org/GLM-4.5",
  "source_scorecard_blog_link": "https://z.ai/blog/glm-4.5",
  "source_weights_link": "https://huggingface.co/zai-org/GLM-4.5-Air",
  "training_tokens": null,
  "updated_at": "2025-09-15T00:00:00.000000+00:00",
  "avg_benchmark_score": 0.6080714285714286
}]