{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e0f362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in ./.venv/lib/python3.12/site-packages (4.15.4)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in ./.venv/lib/python3.12/site-packages (from pymongo) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0569d830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.3.5)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "115b9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1038d61",
   "metadata": {},
   "source": [
    "For loading data into mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8e8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mongo db config\n",
    "def get_mongo_client(uri=\"mongodb://127.0.0.1:27017/\"):\n",
    "\n",
    "    return MongoClient(uri)\n",
    "\n",
    "def get_database(client, db_name=\"admin\"):\n",
    "\n",
    "    return client[db_name]\n",
    "\n",
    "def get_collection(db, collection_name):\n",
    "\n",
    "    return db[collection_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "250e23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"Data\"  #where the extracted root data folder\n",
    "\n",
    "#Mongo DB collections\n",
    "Collections = { 'Benchmark MD', \n",
    "                'LLM overall info',\n",
    "                'Licenses MD',\n",
    "                'Organizations',\n",
    "                'Providers'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc91942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a12ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_json_to_mongo(mongo_uri, db_name, collection_name, folder_dir, id_col):\n",
    "    \"\"\"\n",
    "    Load JSON data including only 1 DATA POINT from a folder into MongoDB.\n",
    "\n",
    "    Parameters:\n",
    "        mongo_uri (str): MongoDB connection URI\n",
    "        db_name (str): Name of the database\n",
    "        collection_name (str): Name of the collection to insert into\n",
    "        folder_dir (str): Path to the folder containing JSON files\n",
    "    \"\"\"\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    print(f\"Reading from: {folder_dir}\")\n",
    "    count = 0\n",
    "\n",
    "    for file in os.listdir(folder_dir):\n",
    "        if not file.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        json_path = os.path.join(folder_dir, file)\n",
    "        data = load_json(json_path)\n",
    "\n",
    "        collection.update_one(\n",
    "            {id_col: data[id_col]},  # avoid duplicates\n",
    "            {\"$set\": data},\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "        print(f\"Inserted: {file}\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"\\n{count} json files loaded into '{collection_name}' collection.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "360606d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_many_json_to_mongo(mongo_uri, db_name, collection_name, folder_dir, id_col):\n",
    "    \"\"\"\n",
    "    Load JSON data including list of data points from a folder into MongoDB.\n",
    "\n",
    "    Parameters:\n",
    "        mongo_uri (str): MongoDB connection URI\n",
    "        db_name (str): Name of the database\n",
    "        collection_name (str): Name of the collection to insert into\n",
    "        folder_dir (str): Path to the folder containing JSON files\n",
    "    \"\"\"\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    print(f\"Reading from: {folder_dir}\")\n",
    "    count = 0\n",
    "\n",
    "    for file in os.listdir(folder_dir):\n",
    "        if not file.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        json_path = os.path.join(folder_dir, file)\n",
    "        data_list = load_json(json_path)  # this is a list []\n",
    "\n",
    "        if not isinstance(data_list, list):\n",
    "            print(f\"{file} is not a list, skipping...\")\n",
    "            continue\n",
    "\n",
    "        for item in data_list:\n",
    "            collection.update_one(\n",
    "                {id_col: item[id_col]},\n",
    "                {\"$set\": item},\n",
    "                upsert=True\n",
    "            )\n",
    "            count += 1\n",
    "\n",
    "    print(f\"\\n{count} json files loaded into '{collection_name}' collection.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36496647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_child_folder_json(mongo_uri, db_name, collection_name, root_dir, child_name, id_col):\n",
    "    \"\"\"\n",
    "    Load master data from each child folder inside root_dir.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    print(f\"Scanning root directory: {root_dir}\")\n",
    "    count = 0\n",
    "\n",
    "    for child in os.listdir(root_dir):\n",
    "        child_path = os.path.join(root_dir, child)\n",
    "\n",
    "        if not os.path.isdir(child_path):\n",
    "            continue\n",
    "\n",
    "        child_file = os.path.join(child_path, child_name)\n",
    "\n",
    "        if not os.path.exists(child_file):\n",
    "            print(f\"No {child_name} in: {child_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(child_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # Insert based on its unique ID\n",
    "        collection.update_one(\n",
    "            {id_col: data[id_col]},\n",
    "            {\"$set\": data},\n",
    "            upsert=True\n",
    "        )\n",
    "\n",
    "        print(f\"Inserted: {child}\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"\\nLoaded {count} {child} files into '{collection_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb76fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm_models(mongo_uri, db_name, collection_name, organizations_dir, id_col):\n",
    "    \"\"\"\n",
    "    Load benchmarks.json (a list of objects) from:\n",
    "        organizations/<org_id>/models/<model_id>/benchmarks.json\n",
    "    \"\"\"\n",
    "\n",
    "    client = MongoClient(mongo_uri)\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "\n",
    "    print(f\"Scanning organizations directory: {organizations_dir}\")\n",
    "    total_items = 0\n",
    "    total_files = 0\n",
    "\n",
    "    for org_folder in os.listdir(organizations_dir):\n",
    "        org_path = os.path.join(organizations_dir, org_folder)\n",
    "        if not os.path.isdir(org_path):\n",
    "            continue\n",
    "\n",
    "        models_dir = os.path.join(org_path, \"models\")\n",
    "        if not os.path.exists(models_dir):\n",
    "            print(f\"No models folder in: {org_folder}\")\n",
    "            continue\n",
    "\n",
    "        for model_folder in os.listdir(models_dir):\n",
    "            model_path = os.path.join(models_dir, model_folder)\n",
    "            if not os.path.isdir(model_path):\n",
    "                continue\n",
    "\n",
    "            json_file = os.path.join(model_path, \"benchmarks.json\")\n",
    "            if not os.path.exists(json_file):\n",
    "                print(f\"No benchmarks.json in: {model_path}\")\n",
    "                continue\n",
    "\n",
    "            # Load list JSON\n",
    "            data_list = load_json(json_file)\n",
    "\n",
    "            if not isinstance(data_list, list):\n",
    "                print(f\"{json_file} is not a list, skipping...\")\n",
    "                continue\n",
    "\n",
    "            total_files += 1\n",
    "\n",
    "            # Insert each item\n",
    "            for item in data_list:\n",
    "\n",
    "                if id_col not in item:\n",
    "                    print(f\"Missing '{id_col}' in item inside {json_file}, skipping...\")\n",
    "                    continue\n",
    "\n",
    "                # Upsert each benchmark record\n",
    "                collection.update_one(\n",
    "                    {id_col: item[id_col]},\n",
    "                    {\"$set\": item},\n",
    "                    upsert=True\n",
    "                )\n",
    "\n",
    "                total_items += 1\n",
    "\n",
    "    print(f\"\\nLoaded {total_items} benchmark records from {total_files} files into '{collection_name}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f96be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = 'mongodb://127.0.0.1:27017/admin2' #this matches in .env\n",
    "DB_NAME = 'admin2' #MongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "416a3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "ROOT_DIR = Path(r\"C:\\Users\\nguye\\OneDrive\\Desktop\\Data\")\n",
    "\n",
    "LICENSE_DIR = ROOT_DIR / \"licenses\"\n",
    "BENCHMARK_DIR =  ROOT_DIR / \"benchmarks\"\n",
    "OVERALL_INFO_DIR =  ROOT_DIR / \"llm_comparison_dataset.csv\"\n",
    "ORGANIZATIONS_DIR =  ROOT_DIR / \"organizations\"\n",
    "PROVIDERS_DIR =  ROOT_DIR / \"providers\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12158926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BENCHMARK_DIR not found: C:\\Users\\nguye\\OneDrive\\Desktop\\Data/benchmarks. Please verify ROOT_DIR or clone the llm-leaderboard-main repository.\n"
     ]
    }
   ],
   "source": [
    "#LOAD BENCHMARK METADATA\n",
    "COLLECTION_NAME = 'Benchmark MD'\n",
    "if BENCHMARK_DIR.exists() and BENCHMARK_DIR.is_dir():\n",
    "    load_single_json_to_mongo(MONGO_URI, DB_NAME, COLLECTION_NAME, BENCHMARK_DIR, 'benchmark_id')\n",
    "else:\n",
    "    print(f\"BENCHMARK_DIR not found: {BENCHMARK_DIR}. Please verify ROOT_DIR or clone the llm-leaderboard-main repository.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8eb531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE_DIR not found: C:\\Users\\nguye\\OneDrive\\Desktop\\Data/licenses. Please verify ROOT_DIR or extract the data.\n"
     ]
    }
   ],
   "source": [
    "#LOAD LICENSES METADATA\n",
    "COLLECTION_NAME = 'Licenses MD'\n",
    "if LICENSE_DIR.exists() and LICENSE_DIR.is_dir():\n",
    "    load_single_json_to_mongo(MONGO_URI, DB_NAME, COLLECTION_NAME, str(LICENSE_DIR), 'license_id')\n",
    "else:\n",
    "    print(f\"LICENSE_DIR not found: {LICENSE_DIR}. Please verify ROOT_DIR or extract the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f5584f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Desktop\\\\Data/llm_comparison_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#LOAD LLM OVERALL INFO\u001b[39;00m\n\u001b[32m      2\u001b[39m COLLECTION_NAME = \u001b[33m'\u001b[39m\u001b[33mLLM overall info\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOVERALL_INFO_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m data_dict = df.to_dict(orient=\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m client = MongoClient(MONGO_URI)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/a-story-of-LLMs-evolution/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/a-story-of-LLMs-evolution/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/a-story-of-LLMs-evolution/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/a-story-of-LLMs-evolution/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/a-story-of-LLMs-evolution/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\nguye\\\\OneDrive\\\\Desktop\\\\Data/llm_comparison_dataset.csv'"
     ]
    }
   ],
   "source": [
    "#LOAD LLM OVERALL INFO\n",
    "COLLECTION_NAME = 'LLM overall info'\n",
    "df = pd.read_csv(OVERALL_INFO_DIR)\n",
    "\n",
    "data_dict = df.to_dict(orient='records')\n",
    "\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "\n",
    "if data_dict:\n",
    "    collection.insert_many(data_dict)\n",
    "    print(f\"Inserted {len(data_dict)} records into '{COLLECTION_NAME}' collection.\")\n",
    "else:\n",
    "    print(\"CSV file is empty. No data inserted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD LLMS FROM ORGANIZATION OVERALL INFO\n",
    "COLLECTION_NAME = 'LLMs in Organizations'\n",
    "load_many_json_to_mongo(MONGO_URI, DB_NAME, COLLECTION_NAME, ORGANIZATIONS_DIR, 'model_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87896b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD LLM SCORE FOR EACH BENCHMARK\n",
    "COLLECTION_NAME = 'LLM Performance'\n",
    "load_llm_models(MONGO_URI, DB_NAME, COLLECTION_NAME, ORGANIZATIONS_DIR, 'model_benchmark_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e51e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD LLMS FROM PROVIDER OVERALL INFO\n",
    "COLLECTION_NAME = 'LLMs in Providers'\n",
    "load_many_json_to_mongo(MONGO_URI, DB_NAME, COLLECTION_NAME, PROVIDERS_DIR, 'provider.json','model_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD LLMS FROM PROVIDER OVERALL INFO\n",
    "COLLECTION_NAME = 'Providers MD'\n",
    "load_child_folder_json(MONGO_URI, DB_NAME, COLLECTION_NAME, PROVIDERS_DIR, 'provider.json','provider_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD LLMS FROM PROVIDER OVERALL INFO\n",
    "COLLECTION_NAME = 'Organizations MD'\n",
    "load_child_folder_json(MONGO_URI, DB_NAME, COLLECTION_NAME, ORGANIZATIONS_DIR, 'organization.json', 'organization_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904a7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Organization & Provider with avg_benchmark_score successfully!\n"
     ]
    }
   ],
   "source": [
    "# MERGE ORGANIZATION & PROVIDER DATA WITH AVERAGE BENCHMARK SCORE\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client[\"admin2\"]\n",
    "\n",
    "org_coll = db[\"LLMs wrt Organizations\"]\n",
    "prov_coll = db[\"LLMs wrt Providers\"]\n",
    "perf_coll = db[\"LLM Performance\"]\n",
    "merged_coll = db[\"LLM Merged Organization and Provider\"]\n",
    "\n",
    "# Step 1: Left join Organization -> Provider\n",
    "pipeline_org = [\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"from\": \"LLMs wrt Providers\",\n",
    "            \"localField\": \"model_id\",\n",
    "            \"foreignField\": \"model_id\",\n",
    "            \"as\": \"provider_data\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"provider_id\": {\"$arrayElemAt\": [\"$provider_data.provider_id\", 0]}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\"provider_data\": 0}  # remove temp array\n",
    "    }\n",
    "]\n",
    "\n",
    "org_docs = list(org_coll.aggregate(pipeline_org))\n",
    "\n",
    "# Step 2: Get provider-only model_ids (not in Organization)\n",
    "org_model_ids = org_coll.distinct(\"model_id\")\n",
    "prov_only_docs = list(prov_coll.find({\"model_id\": {\"$nin\": org_model_ids}}))\n",
    "\n",
    "# Step 3: Merge provider-only docs\n",
    "org_sample_fields = org_coll.find_one() or {}\n",
    "org_field_keys = [k for k in org_sample_fields.keys() if k != \"_id\"]\n",
    "\n",
    "for doc in prov_only_docs:\n",
    "    merged_doc = {\n",
    "        **{k: None for k in org_field_keys},  # all org fields as None\n",
    "        **doc  # overwrite model_id and provider_id from provider\n",
    "    }\n",
    "    org_docs.append(merged_doc)\n",
    "\n",
    "# Step 4: Compute avg_benchmark_score for each doc\n",
    "for doc in org_docs:\n",
    "    scores = list(perf_coll.find({\"model_id\": doc[\"model_id\"]}, {\"normalized_score\": 1, \"_id\": 0}))\n",
    "    if scores:\n",
    "        avg_score = sum(s[\"normalized_score\"] for s in scores) / len(scores)\n",
    "        doc[\"avg_benchmark_score\"] = avg_score\n",
    "    else:\n",
    "        doc[\"avg_benchmark_score\"] = None  # or 0\n",
    "\n",
    "# Step 5: Insert/update merged collection\n",
    "for doc in org_docs:\n",
    "    merged_coll.update_one(\n",
    "        {\"model_id\": doc[\"model_id\"]},\n",
    "        {\"$set\": doc},\n",
    "        upsert=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a0800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'admin2', 'config', 'local']\n"
     ]
    }
   ],
   "source": [
    "#change release_date from string to datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# Fetch all documents\n",
    "for doc in merged_coll.find({\"release_date\": {\"$type\": \"string\"}}):\n",
    "    release_str = doc.get(\"release_date\")\n",
    "    if release_str:\n",
    "        try:\n",
    "            # Convert string to datetime object\n",
    "            release_dt = datetime.fromisoformat(release_str.replace(\"Z\", \"+00:00\"))\n",
    "            merged_coll.update_one(\n",
    "                {\"_id\": doc[\"_id\"]},\n",
    "                {\"$set\": {\"release_date\": release_dt}}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to convert for {doc['_id']}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-story-of-LLMs-evolution (.venv)",
   "language": "python",
   "name": "astory_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
